{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c27752",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:07.29831Z",
     "start_time": "2024-04-30T21:25:05.354584Z"
    },
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:41.955742Z",
     "iopub.status.busy": "2025-04-11T18:30:41.955014Z",
     "iopub.status.idle": "2025-04-11T18:30:51.632223Z",
     "shell.execute_reply": "2025-04-11T18:30:51.631404Z"
    },
    "papermill": {
     "duration": 9.684855,
     "end_time": "2025-04-11T18:30:51.634701",
     "exception": false,
     "start_time": "2025-04-11T18:30:41.949846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from src.dataset.dataset import TrainDataset, TestDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213a77a",
   "metadata": {
    "papermill": {
     "duration": 0.00379,
     "end_time": "2025-04-11T18:30:51.650496",
     "exception": false,
     "start_time": "2025-04-11T18:30:51.646706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vorbereitung des Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a02b1",
   "metadata": {
    "papermill": {
     "duration": 0.003709,
     "end_time": "2025-04-11T18:30:51.684240",
     "exception": false,
     "start_time": "2025-04-11T18:30:51.680531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Metadaten laden und Dataloader vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5fc9db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:34.532017Z",
     "start_time": "2024-04-30T21:25:32.615562Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:51.693628Z",
     "iopub.status.busy": "2025-04-11T18:30:51.693293Z",
     "iopub.status.idle": "2025-04-11T18:30:57.431246Z",
     "shell.execute_reply": "2025-04-11T18:30:57.429814Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.745402,
     "end_time": "2025-04-11T18:30:57.433545",
     "exception": false,
     "start_time": "2025-04-11T18:30:51.688143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 14784\n",
      "Train dataset size: 88987\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "batch_size = 128\n",
    "num_workers = 8\n",
    "model_name = \"ResNet-18-Eurosat-BLU\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Load Test metadata\n",
    "test_data_path = \"data/SatellitePatches/PA-test/\"\n",
    "test_metadata_path = \"data/GLC25_PA_metadata_test.csv\"\n",
    "test_metadata = pd.read_csv(test_metadata_path)\n",
    "test_dataset = TestDataset(test_data_path, test_metadata, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Load Training metadata\n",
    "train_data_path = \"data/SatellitePatches/PA-train\"\n",
    "train_metadata_path = \"data/GLC25_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e8b59",
   "metadata": {
    "papermill": {
     "duration": 0.004179,
     "end_time": "2025-04-11T18:30:57.441900",
     "exception": false,
     "start_time": "2025-04-11T18:30:57.437721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vortrainiertes Modell Laden sowie Gerät für Berechnungen auswählen\n",
    "\n",
    "### Gerät auswählen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "877d7197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T21:25:31.014067Z",
     "start_time": "2024-04-30T21:25:31.01006Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:57.450727Z",
     "iopub.status.busy": "2025-04-11T18:30:57.450448Z",
     "iopub.status.idle": "2025-04-11T18:30:57.528529Z",
     "shell.execute_reply": "2025-04-11T18:30:57.527509Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.084742,
     "end_time": "2025-04-11T18:30:57.530654",
     "exception": false,
     "start_time": "2025-04-11T18:30:57.445912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "from src.helpers import select_device\n",
    "\n",
    "# Check if cuda is available\n",
    "device = select_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 25\n",
    "positive_weigh_factor = 1.0\n",
    "num_classes = 11255 # Number of all unique classes within the PO and PA data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8cca8",
   "metadata": {},
   "source": [
    "### Modell instanziieren und zum Gerät verschieben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a7f756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:57.539829Z",
     "iopub.status.busy": "2025-04-11T18:30:57.539525Z",
     "iopub.status.idle": "2025-04-11T18:30:58.466528Z",
     "shell.execute_reply": "2025-04-11T18:30:58.465645Z"
    },
    "papermill": {
     "duration": 0.933587,
     "end_time": "2025-04-11T18:30:58.468395",
     "exception": false,
     "start_time": "2025-04-11T18:30:57.534808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.model.ResNets import ResNet18, ResNet50\n",
    "\n",
    "\n",
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904fb3d5",
   "metadata": {},
   "source": [
    "### Modell visualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ce6243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act1): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (drop_block): Identity()\n",
      "        (act1): ReLU(inplace=True)\n",
      "        (aa): Identity()\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (act2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "    (fc): Linear(in_features=512, out_features=11255, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180e72a",
   "metadata": {},
   "source": [
    "### Zum Zweck der Reproduzierbarkeit Seed setzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7946956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:58.478049Z",
     "iopub.status.busy": "2025-04-11T18:30:58.477780Z",
     "iopub.status.idle": "2025-04-11T18:30:58.482847Z",
     "shell.execute_reply": "2025-04-11T18:30:58.482065Z"
    },
    "papermill": {
     "duration": 0.011755,
     "end_time": "2025-04-11T18:30:58.484569",
     "exception": false,
     "start_time": "2025-04-11T18:30:58.472814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from src.helpers import set_seed\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd18b4b",
   "metadata": {
    "papermill": {
     "duration": 0.003826,
     "end_time": "2025-04-11T18:30:58.492394",
     "exception": false,
     "start_time": "2025-04-11T18:30:58.488568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop\n",
    "\n",
    "### Erst mit PA-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helpers import train_loop\n",
    "\n",
    "train_loop(model=model,\n",
    "           train_loader=train_loader,\n",
    "           optimizer=optimizer,\n",
    "           device=device,\n",
    "           scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5785275f",
   "metadata": {},
   "source": [
    "### Modell im Evaluierungsmodus speichern und testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9604d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), f\"{model_name}\")\n",
    "\n",
    "from src.helpers import test_loop\n",
    "\n",
    "surveys, top_k_indices = test_loop(model, test_loader, device)\n",
    "\n",
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'surveyId': surveys,\n",
    "        'predictions': data_concatenated,\n",
    "    }\n",
    ").to_csv(f\"csv_submissions/{model_name}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769d750a",
   "metadata": {},
   "source": [
    "### Modell wieder in Trainingsmodus versetzen, Dataloader mit PO-Dataset instanziieren und erneut trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf61a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "# Load Training metadata\n",
    "train_data_path = \"data/SatellitePatches/po/output/TIFF_64\"\n",
    "train_metadata_path = \"data/GLC25_PO_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, transform=transform, grid_length=0.01)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa8044",
   "metadata": {},
   "source": [
    "### Training mit PO-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146d3632",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-30T21:25:34.536634Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-11T18:30:58.502389Z",
     "iopub.status.busy": "2025-04-11T18:30:58.501748Z",
     "iopub.status.idle": "2025-04-12T04:14:09.901308Z",
     "shell.execute_reply": "2025-04-12T04:14:09.900211Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 34991.420477,
     "end_time": "2025-04-12T04:14:09.917095",
     "exception": false,
     "start_time": "2025-04-11T18:30:58.496618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25 epochs started.\n",
      "Epoch 1/25, Batch 0/696, Loss: 0.7137155532836914\n",
      "Epoch 1/25, Batch 348/696, Loss: 0.014051029458642006\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 1, '_step_count': 2, '_get_lr_called_within_step': False, '_last_lr': [9.96057350657239e-05]}\n",
      "Epoch 2/25, Batch 0/696, Loss: 0.008304858580231667\n",
      "Epoch 2/25, Batch 348/696, Loss: 0.00687360018491745\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 2, '_step_count': 3, '_get_lr_called_within_step': False, '_last_lr': [9.842915805643155e-05]}\n",
      "Epoch 3/25, Batch 0/696, Loss: 0.0066201346926391125\n",
      "Epoch 3/25, Batch 348/696, Loss: 0.00669951131567359\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 3, '_step_count': 4, '_get_lr_called_within_step': False, '_last_lr': [9.648882429441257e-05]}\n",
      "Epoch 4/25, Batch 0/696, Loss: 0.006014692131429911\n",
      "Epoch 4/25, Batch 348/696, Loss: 0.006399218924343586\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 4, '_step_count': 5, '_get_lr_called_within_step': False, '_last_lr': [9.381533400219318e-05]}\n",
      "Epoch 5/25, Batch 0/696, Loss: 0.006009676028043032\n",
      "Epoch 5/25, Batch 348/696, Loss: 0.006242614705115557\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 5, '_step_count': 6, '_get_lr_called_within_step': False, '_last_lr': [9.045084971874738e-05]}\n",
      "Epoch 6/25, Batch 0/696, Loss: 0.007009332533925772\n",
      "Epoch 6/25, Batch 348/696, Loss: 0.006799024995416403\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 6, '_step_count': 7, '_get_lr_called_within_step': False, '_last_lr': [8.644843137107059e-05]}\n",
      "Epoch 7/25, Batch 0/696, Loss: 0.005897109396755695\n",
      "Epoch 7/25, Batch 348/696, Loss: 0.0061729224398732185\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 7, '_step_count': 8, '_get_lr_called_within_step': False, '_last_lr': [8.18711994874345e-05]}\n",
      "Epoch 8/25, Batch 0/696, Loss: 0.005775672383606434\n",
      "Epoch 8/25, Batch 348/696, Loss: 0.0060327500104904175\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 8, '_step_count': 9, '_get_lr_called_within_step': False, '_last_lr': [7.679133974894983e-05]}\n",
      "Epoch 9/25, Batch 0/696, Loss: 0.006646450608968735\n",
      "Epoch 9/25, Batch 348/696, Loss: 0.005846892949193716\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 9, '_step_count': 10, '_get_lr_called_within_step': False, '_last_lr': [7.128896457825363e-05]}\n",
      "Epoch 10/25, Batch 0/696, Loss: 0.005755322519689798\n",
      "Epoch 10/25, Batch 348/696, Loss: 0.0060102506540715694\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 10, '_step_count': 11, '_get_lr_called_within_step': False, '_last_lr': [6.545084971874737e-05]}\n",
      "Epoch 11/25, Batch 0/696, Loss: 0.005538596771657467\n",
      "Epoch 11/25, Batch 348/696, Loss: 0.005741821601986885\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 11, '_step_count': 12, '_get_lr_called_within_step': False, '_last_lr': [5.936906572928624e-05]}\n",
      "Epoch 12/25, Batch 0/696, Loss: 0.005652479827404022\n",
      "Epoch 12/25, Batch 348/696, Loss: 0.0055527095682919025\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 12, '_step_count': 13, '_get_lr_called_within_step': False, '_last_lr': [5.313952597646568e-05]}\n",
      "Epoch 13/25, Batch 0/696, Loss: 0.005692133214324713\n",
      "Epoch 13/25, Batch 348/696, Loss: 0.005361364688724279\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 13, '_step_count': 14, '_get_lr_called_within_step': False, '_last_lr': [4.6860474023534335e-05]}\n",
      "Epoch 14/25, Batch 0/696, Loss: 0.005893774330615997\n",
      "Epoch 14/25, Batch 348/696, Loss: 0.00518720131367445\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 14, '_step_count': 15, '_get_lr_called_within_step': False, '_last_lr': [4.0630934270713774e-05]}\n",
      "Epoch 15/25, Batch 0/696, Loss: 0.005731513258069754\n",
      "Epoch 15/25, Batch 348/696, Loss: 0.005765817128121853\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 15, '_step_count': 16, '_get_lr_called_within_step': False, '_last_lr': [3.454915028125265e-05]}\n",
      "Epoch 16/25, Batch 0/696, Loss: 0.005271900445222855\n",
      "Epoch 16/25, Batch 348/696, Loss: 0.005047067534178495\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 16, '_step_count': 17, '_get_lr_called_within_step': False, '_last_lr': [2.871103542174637e-05]}\n",
      "Epoch 17/25, Batch 0/696, Loss: 0.00512212747707963\n",
      "Epoch 17/25, Batch 348/696, Loss: 0.005520372651517391\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 17, '_step_count': 18, '_get_lr_called_within_step': False, '_last_lr': [2.3208660251050158e-05]}\n",
      "Epoch 18/25, Batch 0/696, Loss: 0.005191727075725794\n",
      "Epoch 18/25, Batch 348/696, Loss: 0.00540420925244689\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 18, '_step_count': 19, '_get_lr_called_within_step': False, '_last_lr': [1.8128800512565513e-05]}\n",
      "Epoch 19/25, Batch 0/696, Loss: 0.005218090023845434\n",
      "Epoch 19/25, Batch 348/696, Loss: 0.0057342397049069405\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 19, '_step_count': 20, '_get_lr_called_within_step': False, '_last_lr': [1.3551568628929434e-05]}\n",
      "Epoch 20/25, Batch 0/696, Loss: 0.005833759438246489\n",
      "Epoch 20/25, Batch 348/696, Loss: 0.005394212435930967\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 20, '_step_count': 21, '_get_lr_called_within_step': False, '_last_lr': [9.549150281252635e-06]}\n",
      "Epoch 21/25, Batch 0/696, Loss: 0.005199859384447336\n",
      "Epoch 21/25, Batch 348/696, Loss: 0.005408810451626778\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 21, '_step_count': 22, '_get_lr_called_within_step': False, '_last_lr': [6.184665997806822e-06]}\n",
      "Epoch 22/25, Batch 0/696, Loss: 0.004968635272234678\n",
      "Epoch 22/25, Batch 348/696, Loss: 0.005346153862774372\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 22, '_step_count': 23, '_get_lr_called_within_step': False, '_last_lr': [3.5111757055874332e-06]}\n",
      "Epoch 23/25, Batch 0/696, Loss: 0.005318863783031702\n",
      "Epoch 23/25, Batch 348/696, Loss: 0.005451294593513012\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 23, '_step_count': 24, '_get_lr_called_within_step': False, '_last_lr': [1.570841943568452e-06]}\n",
      "Epoch 24/25, Batch 0/696, Loss: 0.005249144975095987\n",
      "Epoch 24/25, Batch 348/696, Loss: 0.005788755137473345\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 24, '_step_count': 25, '_get_lr_called_within_step': False, '_last_lr': [3.942649342761118e-07]}\n",
      "Epoch 25/25, Batch 0/696, Loss: 0.005053193774074316\n",
      "Epoch 25/25, Batch 348/696, Loss: 0.005564709194004536\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 25, '_step_count': 26, '_get_lr_called_within_step': False, '_last_lr': [0.0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [01:22<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25 epochs started.\n",
      "Epoch 1/25, Batch 0/30044, Loss: 0.013165660202503204\n",
      "Epoch 1/25, Batch 348/30044, Loss: 0.014004808850586414\n",
      "Epoch 1/25, Batch 696/30044, Loss: 0.014226202853024006\n",
      "Epoch 1/25, Batch 1044/30044, Loss: 0.01219420600682497\n",
      "Epoch 1/25, Batch 1392/30044, Loss: 0.014183687046170235\n",
      "Epoch 1/25, Batch 1740/30044, Loss: 0.015224668197333813\n",
      "Epoch 1/25, Batch 2088/30044, Loss: 0.01579141430556774\n",
      "Epoch 1/25, Batch 2436/30044, Loss: 0.01270377542823553\n",
      "Epoch 1/25, Batch 2784/30044, Loss: 0.013817595317959785\n",
      "Epoch 1/25, Batch 3132/30044, Loss: 0.01760709285736084\n",
      "Epoch 1/25, Batch 3480/30044, Loss: 0.016772344708442688\n",
      "Epoch 1/25, Batch 3828/30044, Loss: 0.0129721499979496\n",
      "Epoch 1/25, Batch 4176/30044, Loss: 0.016191553324460983\n",
      "Epoch 1/25, Batch 4524/30044, Loss: 0.01632431522011757\n",
      "Epoch 1/25, Batch 4872/30044, Loss: 0.01413026824593544\n",
      "Epoch 1/25, Batch 5220/30044, Loss: 0.01404015813022852\n",
      "Epoch 1/25, Batch 5568/30044, Loss: 0.013363116420805454\n",
      "Epoch 1/25, Batch 5916/30044, Loss: 0.015424724668264389\n",
      "Epoch 1/25, Batch 6264/30044, Loss: 0.01489371620118618\n",
      "Epoch 1/25, Batch 6612/30044, Loss: 0.011073562316596508\n",
      "Epoch 1/25, Batch 6960/30044, Loss: 0.014606253243982792\n",
      "Epoch 1/25, Batch 7308/30044, Loss: 0.01499023474752903\n",
      "Epoch 1/25, Batch 7656/30044, Loss: 0.014515604823827744\n",
      "Epoch 1/25, Batch 8004/30044, Loss: 0.015954917296767235\n",
      "Epoch 1/25, Batch 8352/30044, Loss: 0.01599537394940853\n",
      "Epoch 1/25, Batch 8700/30044, Loss: 0.014820558950304985\n",
      "Epoch 1/25, Batch 9048/30044, Loss: 0.012419546023011208\n",
      "Epoch 1/25, Batch 9396/30044, Loss: 0.017156971618533134\n",
      "Epoch 1/25, Batch 9744/30044, Loss: 0.012585105374455452\n",
      "Epoch 1/25, Batch 10092/30044, Loss: 0.012346412055194378\n",
      "Epoch 1/25, Batch 10440/30044, Loss: 0.01584998145699501\n",
      "Epoch 1/25, Batch 10788/30044, Loss: 0.01781909540295601\n",
      "Epoch 1/25, Batch 11136/30044, Loss: 0.012652396224439144\n",
      "Epoch 1/25, Batch 11484/30044, Loss: 0.013091051951050758\n",
      "Epoch 1/25, Batch 11832/30044, Loss: 0.013608306646347046\n",
      "Epoch 1/25, Batch 12180/30044, Loss: 0.017801225185394287\n",
      "Epoch 1/25, Batch 12528/30044, Loss: 0.014870550483465195\n",
      "Epoch 1/25, Batch 12876/30044, Loss: 0.012558473274111748\n",
      "Epoch 1/25, Batch 13224/30044, Loss: 0.019753674045205116\n",
      "Epoch 1/25, Batch 13572/30044, Loss: 0.011576605960726738\n",
      "Epoch 1/25, Batch 13920/30044, Loss: 0.01692230999469757\n",
      "Epoch 1/25, Batch 14268/30044, Loss: 0.013185377232730389\n",
      "Epoch 1/25, Batch 14616/30044, Loss: 0.014162966050207615\n",
      "Epoch 1/25, Batch 14964/30044, Loss: 0.011687449179589748\n",
      "Epoch 1/25, Batch 15312/30044, Loss: 0.013226296752691269\n",
      "Epoch 1/25, Batch 15660/30044, Loss: 0.015124439261853695\n",
      "Epoch 1/25, Batch 16008/30044, Loss: 0.013516074046492577\n",
      "Epoch 1/25, Batch 16356/30044, Loss: 0.01539548859000206\n",
      "Epoch 1/25, Batch 16704/30044, Loss: 0.01441840548068285\n",
      "Epoch 1/25, Batch 17052/30044, Loss: 0.013012655079364777\n",
      "Epoch 1/25, Batch 17400/30044, Loss: 0.012301729992032051\n",
      "Epoch 1/25, Batch 17748/30044, Loss: 0.014260882511734962\n",
      "Epoch 1/25, Batch 18096/30044, Loss: 0.011645468883216381\n",
      "Epoch 1/25, Batch 18444/30044, Loss: 0.015912452712655067\n",
      "Epoch 1/25, Batch 18792/30044, Loss: 0.013773608021438122\n",
      "Epoch 1/25, Batch 19140/30044, Loss: 0.01627807319164276\n",
      "Epoch 1/25, Batch 19488/30044, Loss: 0.014696596190333366\n",
      "Epoch 1/25, Batch 19836/30044, Loss: 0.01405416801571846\n",
      "Epoch 1/25, Batch 20184/30044, Loss: 0.014012130908668041\n",
      "Epoch 1/25, Batch 20532/30044, Loss: 0.013604874722659588\n",
      "Epoch 1/25, Batch 20880/30044, Loss: 0.012626832351088524\n",
      "Epoch 1/25, Batch 21228/30044, Loss: 0.015959812328219414\n",
      "Epoch 1/25, Batch 21576/30044, Loss: 0.017704907804727554\n",
      "Epoch 1/25, Batch 21924/30044, Loss: 0.014450774528086185\n",
      "Epoch 1/25, Batch 22272/30044, Loss: 0.01636558771133423\n",
      "Epoch 1/25, Batch 22620/30044, Loss: 0.013769689947366714\n",
      "Epoch 1/25, Batch 22968/30044, Loss: 0.013498558662831783\n",
      "Epoch 1/25, Batch 23316/30044, Loss: 0.013597418554127216\n",
      "Epoch 1/25, Batch 23664/30044, Loss: 0.01686687208712101\n",
      "Epoch 1/25, Batch 24012/30044, Loss: 0.013704353012144566\n",
      "Epoch 1/25, Batch 24360/30044, Loss: 0.010400733910501003\n",
      "Epoch 1/25, Batch 24708/30044, Loss: 0.014605876058340073\n",
      "Epoch 1/25, Batch 25056/30044, Loss: 0.015465655364096165\n",
      "Epoch 1/25, Batch 25404/30044, Loss: 0.014198526740074158\n",
      "Epoch 1/25, Batch 25752/30044, Loss: 0.011983172968029976\n",
      "Epoch 1/25, Batch 26100/30044, Loss: 0.013544159941375256\n",
      "Epoch 1/25, Batch 26448/30044, Loss: 0.014107386581599712\n",
      "Epoch 1/25, Batch 26796/30044, Loss: 0.013194775208830833\n",
      "Epoch 1/25, Batch 27144/30044, Loss: 0.01278478279709816\n",
      "Epoch 1/25, Batch 27492/30044, Loss: 0.01336592435836792\n",
      "Epoch 1/25, Batch 27840/30044, Loss: 0.016925737261772156\n",
      "Epoch 1/25, Batch 28188/30044, Loss: 0.012898907996714115\n",
      "Epoch 1/25, Batch 28536/30044, Loss: 0.01595992036163807\n",
      "Epoch 1/25, Batch 28884/30044, Loss: 0.012330962345004082\n",
      "Epoch 1/25, Batch 29232/30044, Loss: 0.012757670134305954\n",
      "Epoch 1/25, Batch 29580/30044, Loss: 0.011985193006694317\n",
      "Epoch 1/25, Batch 29928/30044, Loss: 0.00986457522958517\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 26, '_step_count': 27, '_get_lr_called_within_step': False, '_last_lr': [3.942649342761062e-07]}\n",
      "Epoch 2/25, Batch 0/30044, Loss: 0.01239137351512909\n",
      "Epoch 2/25, Batch 348/30044, Loss: 0.017758669331669807\n",
      "Epoch 2/25, Batch 696/30044, Loss: 0.013791377656161785\n",
      "Epoch 2/25, Batch 1044/30044, Loss: 0.019785316661000252\n",
      "Epoch 2/25, Batch 1392/30044, Loss: 0.014153759926557541\n",
      "Epoch 2/25, Batch 1740/30044, Loss: 0.015278119593858719\n",
      "Epoch 2/25, Batch 2088/30044, Loss: 0.015866611152887344\n",
      "Epoch 2/25, Batch 2436/30044, Loss: 0.013150999322533607\n",
      "Epoch 2/25, Batch 2784/30044, Loss: 0.011624128557741642\n",
      "Epoch 2/25, Batch 3132/30044, Loss: 0.013176812790334225\n",
      "Epoch 2/25, Batch 3480/30044, Loss: 0.013087494298815727\n",
      "Epoch 2/25, Batch 3828/30044, Loss: 0.014720720238983631\n",
      "Epoch 2/25, Batch 4176/30044, Loss: 0.016269415616989136\n",
      "Epoch 2/25, Batch 4524/30044, Loss: 0.015022844076156616\n",
      "Epoch 2/25, Batch 4872/30044, Loss: 0.011613928712904453\n",
      "Epoch 2/25, Batch 5220/30044, Loss: 0.012963558547198772\n",
      "Epoch 2/25, Batch 5568/30044, Loss: 0.012525276280939579\n",
      "Epoch 2/25, Batch 5916/30044, Loss: 0.013996584340929985\n",
      "Epoch 2/25, Batch 6264/30044, Loss: 0.013604962266981602\n",
      "Epoch 2/25, Batch 6612/30044, Loss: 0.014651374891400337\n",
      "Epoch 2/25, Batch 6960/30044, Loss: 0.013006309047341347\n",
      "Epoch 2/25, Batch 7308/30044, Loss: 0.013571090064942837\n",
      "Epoch 2/25, Batch 7656/30044, Loss: 0.014785069972276688\n",
      "Epoch 2/25, Batch 8004/30044, Loss: 0.01214537676423788\n",
      "Epoch 2/25, Batch 8352/30044, Loss: 0.012204352766275406\n",
      "Epoch 2/25, Batch 8700/30044, Loss: 0.011487322859466076\n",
      "Epoch 2/25, Batch 9048/30044, Loss: 0.011884825304150581\n",
      "Epoch 2/25, Batch 9396/30044, Loss: 0.013151072897017002\n",
      "Epoch 2/25, Batch 9744/30044, Loss: 0.012394036166369915\n",
      "Epoch 2/25, Batch 10092/30044, Loss: 0.012483209371566772\n",
      "Epoch 2/25, Batch 10440/30044, Loss: 0.013554106466472149\n",
      "Epoch 2/25, Batch 10788/30044, Loss: 0.011605218052864075\n",
      "Epoch 2/25, Batch 11136/30044, Loss: 0.012827655300498009\n",
      "Epoch 2/25, Batch 11484/30044, Loss: 0.012160973623394966\n",
      "Epoch 2/25, Batch 11832/30044, Loss: 0.014741243794560432\n",
      "Epoch 2/25, Batch 12180/30044, Loss: 0.012384804897010326\n",
      "Epoch 2/25, Batch 12528/30044, Loss: 0.011605537496507168\n",
      "Epoch 2/25, Batch 12876/30044, Loss: 0.01267282385379076\n",
      "Epoch 2/25, Batch 13224/30044, Loss: 0.008722837083041668\n",
      "Epoch 2/25, Batch 13572/30044, Loss: 0.011423909105360508\n",
      "Epoch 2/25, Batch 13920/30044, Loss: 0.01116205845028162\n",
      "Epoch 2/25, Batch 14268/30044, Loss: 0.010969462804496288\n",
      "Epoch 2/25, Batch 14616/30044, Loss: 0.009840467013418674\n",
      "Epoch 2/25, Batch 14964/30044, Loss: 0.011481814086437225\n",
      "Epoch 2/25, Batch 15312/30044, Loss: 0.012226752005517483\n",
      "Epoch 2/25, Batch 15660/30044, Loss: 0.010398966260254383\n",
      "Epoch 2/25, Batch 16008/30044, Loss: 0.00827229954302311\n",
      "Epoch 2/25, Batch 16356/30044, Loss: 0.011998009867966175\n",
      "Epoch 2/25, Batch 16704/30044, Loss: 0.009483132511377335\n",
      "Epoch 2/25, Batch 17052/30044, Loss: 0.01075128186494112\n",
      "Epoch 2/25, Batch 17400/30044, Loss: 0.01050062756985426\n",
      "Epoch 2/25, Batch 17748/30044, Loss: 0.008732851594686508\n",
      "Epoch 2/25, Batch 18096/30044, Loss: 0.010264948941767216\n",
      "Epoch 2/25, Batch 18444/30044, Loss: 0.010599978268146515\n",
      "Epoch 2/25, Batch 18792/30044, Loss: 0.011960518546402454\n",
      "Epoch 2/25, Batch 19140/30044, Loss: 0.011414225213229656\n",
      "Epoch 2/25, Batch 19488/30044, Loss: 0.00950926449149847\n",
      "Epoch 2/25, Batch 19836/30044, Loss: 0.010971669107675552\n",
      "Epoch 2/25, Batch 20184/30044, Loss: 0.008483744226396084\n",
      "Epoch 2/25, Batch 20532/30044, Loss: 0.009950815699994564\n",
      "Epoch 2/25, Batch 20880/30044, Loss: 0.01125104445964098\n",
      "Epoch 2/25, Batch 21228/30044, Loss: 0.0108025586232543\n",
      "Epoch 2/25, Batch 21576/30044, Loss: 0.009737426415085793\n",
      "Epoch 2/25, Batch 21924/30044, Loss: 0.009528492577373981\n",
      "Epoch 2/25, Batch 22272/30044, Loss: 0.010337013751268387\n",
      "Epoch 2/25, Batch 22620/30044, Loss: 0.01008780300617218\n",
      "Epoch 2/25, Batch 22968/30044, Loss: 0.010627827607095242\n",
      "Epoch 2/25, Batch 23316/30044, Loss: 0.0112943509593606\n",
      "Epoch 2/25, Batch 23664/30044, Loss: 0.010692233219742775\n",
      "Epoch 2/25, Batch 24012/30044, Loss: 0.010564811527729034\n",
      "Epoch 2/25, Batch 24360/30044, Loss: 0.01013142429292202\n",
      "Epoch 2/25, Batch 24708/30044, Loss: 0.010208466090261936\n",
      "Epoch 2/25, Batch 25056/30044, Loss: 0.010906608775258064\n",
      "Epoch 2/25, Batch 25404/30044, Loss: 0.009361919946968555\n",
      "Epoch 2/25, Batch 25752/30044, Loss: 0.010133741423487663\n",
      "Epoch 2/25, Batch 26100/30044, Loss: 0.010361990891397\n",
      "Epoch 2/25, Batch 26448/30044, Loss: 0.010358994826674461\n",
      "Epoch 2/25, Batch 26796/30044, Loss: 0.009553033858537674\n",
      "Epoch 2/25, Batch 27144/30044, Loss: 0.010583584196865559\n",
      "Epoch 2/25, Batch 27492/30044, Loss: 0.010978734120726585\n",
      "Epoch 2/25, Batch 27840/30044, Loss: 0.008822698146104813\n",
      "Epoch 2/25, Batch 28188/30044, Loss: 0.009009248577058315\n",
      "Epoch 2/25, Batch 28536/30044, Loss: 0.009847019799053669\n",
      "Epoch 2/25, Batch 28884/30044, Loss: 0.011838416568934917\n",
      "Epoch 2/25, Batch 29232/30044, Loss: 0.008861319161951542\n",
      "Epoch 2/25, Batch 29580/30044, Loss: 0.010475165210664272\n",
      "Epoch 2/25, Batch 29928/30044, Loss: 0.008820503950119019\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 27, '_step_count': 28, '_get_lr_called_within_step': False, '_last_lr': [1.5708419435684407e-06]}\n",
      "Epoch 3/25, Batch 0/30044, Loss: 0.00924585945904255\n",
      "Epoch 3/25, Batch 348/30044, Loss: 0.009313699789345264\n",
      "Epoch 3/25, Batch 696/30044, Loss: 0.010255685076117516\n",
      "Epoch 3/25, Batch 1044/30044, Loss: 0.009860919788479805\n",
      "Epoch 3/25, Batch 1392/30044, Loss: 0.009112263098359108\n",
      "Epoch 3/25, Batch 1740/30044, Loss: 0.008694698102772236\n",
      "Epoch 3/25, Batch 2088/30044, Loss: 0.010084526613354683\n",
      "Epoch 3/25, Batch 2436/30044, Loss: 0.008794095367193222\n",
      "Epoch 3/25, Batch 2784/30044, Loss: 0.010670008137822151\n",
      "Epoch 3/25, Batch 3132/30044, Loss: 0.00982146430760622\n",
      "Epoch 3/25, Batch 3480/30044, Loss: 0.010232352651655674\n",
      "Epoch 3/25, Batch 3828/30044, Loss: 0.00931178592145443\n",
      "Epoch 3/25, Batch 4176/30044, Loss: 0.008907734416425228\n",
      "Epoch 3/25, Batch 4524/30044, Loss: 0.01129663735628128\n",
      "Epoch 3/25, Batch 4872/30044, Loss: 0.00839341152459383\n",
      "Epoch 3/25, Batch 5220/30044, Loss: 0.01153977494686842\n",
      "Epoch 3/25, Batch 5568/30044, Loss: 0.008772142231464386\n",
      "Epoch 3/25, Batch 5916/30044, Loss: 0.009061797522008419\n",
      "Epoch 3/25, Batch 6264/30044, Loss: 0.008975527249276638\n",
      "Epoch 3/25, Batch 6612/30044, Loss: 0.008557689376175404\n",
      "Epoch 3/25, Batch 6960/30044, Loss: 0.009072980843484402\n",
      "Epoch 3/25, Batch 7308/30044, Loss: 0.010996518656611443\n",
      "Epoch 3/25, Batch 7656/30044, Loss: 0.008149662986397743\n",
      "Epoch 3/25, Batch 8004/30044, Loss: 0.010103086940944195\n",
      "Epoch 3/25, Batch 8352/30044, Loss: 0.009238970465958118\n",
      "Epoch 3/25, Batch 8700/30044, Loss: 0.009814626537263393\n",
      "Epoch 3/25, Batch 9048/30044, Loss: 0.009164863266050816\n",
      "Epoch 3/25, Batch 9396/30044, Loss: 0.008150090463459492\n",
      "Epoch 3/25, Batch 9744/30044, Loss: 0.009281564503908157\n",
      "Epoch 3/25, Batch 10092/30044, Loss: 0.010778707452118397\n",
      "Epoch 3/25, Batch 10440/30044, Loss: 0.009936479851603508\n",
      "Epoch 3/25, Batch 10788/30044, Loss: 0.00875176303088665\n",
      "Epoch 3/25, Batch 11136/30044, Loss: 0.01139342412352562\n",
      "Epoch 3/25, Batch 11484/30044, Loss: 0.01004079356789589\n",
      "Epoch 3/25, Batch 11832/30044, Loss: 0.012052074074745178\n",
      "Epoch 3/25, Batch 12180/30044, Loss: 0.010586496442556381\n",
      "Epoch 3/25, Batch 12528/30044, Loss: 0.009516776539385319\n",
      "Epoch 3/25, Batch 12876/30044, Loss: 0.008336780592799187\n",
      "Epoch 3/25, Batch 13224/30044, Loss: 0.009383109398186207\n",
      "Epoch 3/25, Batch 13572/30044, Loss: 0.009079970419406891\n",
      "Epoch 3/25, Batch 13920/30044, Loss: 0.007408984936773777\n",
      "Epoch 3/25, Batch 14268/30044, Loss: 0.008563398383557796\n",
      "Epoch 3/25, Batch 14616/30044, Loss: 0.01129609439522028\n",
      "Epoch 3/25, Batch 14964/30044, Loss: 0.009538120590150356\n",
      "Epoch 3/25, Batch 15312/30044, Loss: 0.009565820917487144\n",
      "Epoch 3/25, Batch 15660/30044, Loss: 0.00933071132749319\n",
      "Epoch 3/25, Batch 16008/30044, Loss: 0.007213778328150511\n",
      "Epoch 3/25, Batch 16356/30044, Loss: 0.008546528406441212\n",
      "Epoch 3/25, Batch 16704/30044, Loss: 0.010084188543260098\n",
      "Epoch 3/25, Batch 17052/30044, Loss: 0.008821504190564156\n",
      "Epoch 3/25, Batch 17400/30044, Loss: 0.010201412253081799\n",
      "Epoch 3/25, Batch 17748/30044, Loss: 0.009058435447514057\n",
      "Epoch 3/25, Batch 18096/30044, Loss: 0.01133971568197012\n",
      "Epoch 3/25, Batch 18444/30044, Loss: 0.010616295970976353\n",
      "Epoch 3/25, Batch 18792/30044, Loss: 0.010102453641593456\n",
      "Epoch 3/25, Batch 19140/30044, Loss: 0.008083117194473743\n",
      "Epoch 3/25, Batch 19488/30044, Loss: 0.009482229128479958\n",
      "Epoch 3/25, Batch 19836/30044, Loss: 0.009848141111433506\n",
      "Epoch 3/25, Batch 20184/30044, Loss: 0.008820299990475178\n",
      "Epoch 3/25, Batch 20532/30044, Loss: 0.010155539959669113\n",
      "Epoch 3/25, Batch 20880/30044, Loss: 0.01140906848013401\n",
      "Epoch 3/25, Batch 21228/30044, Loss: 0.00926788616925478\n",
      "Epoch 3/25, Batch 21576/30044, Loss: 0.00847853347659111\n",
      "Epoch 3/25, Batch 21924/30044, Loss: 0.008997523225843906\n",
      "Epoch 3/25, Batch 22272/30044, Loss: 0.00883339811116457\n",
      "Epoch 3/25, Batch 22620/30044, Loss: 0.009097825735807419\n",
      "Epoch 3/25, Batch 22968/30044, Loss: 0.008883984759449959\n",
      "Epoch 3/25, Batch 23316/30044, Loss: 0.010920879431068897\n",
      "Epoch 3/25, Batch 23664/30044, Loss: 0.010596361942589283\n",
      "Epoch 3/25, Batch 24012/30044, Loss: 0.010359756648540497\n",
      "Epoch 3/25, Batch 24360/30044, Loss: 0.008120596408843994\n",
      "Epoch 3/25, Batch 24708/30044, Loss: 0.00944381020963192\n",
      "Epoch 3/25, Batch 25056/30044, Loss: 0.00871309358626604\n",
      "Epoch 3/25, Batch 25404/30044, Loss: 0.008004470728337765\n",
      "Epoch 3/25, Batch 25752/30044, Loss: 0.007839749567210674\n",
      "Epoch 3/25, Batch 26100/30044, Loss: 0.0111563540995121\n",
      "Epoch 3/25, Batch 26448/30044, Loss: 0.009486639872193336\n",
      "Epoch 3/25, Batch 26796/30044, Loss: 0.009793674573302269\n",
      "Epoch 3/25, Batch 27144/30044, Loss: 0.009789498522877693\n",
      "Epoch 3/25, Batch 27492/30044, Loss: 0.009837997145950794\n",
      "Epoch 3/25, Batch 27840/30044, Loss: 0.009736290201544762\n",
      "Epoch 3/25, Batch 28188/30044, Loss: 0.007179217878729105\n",
      "Epoch 3/25, Batch 28536/30044, Loss: 0.009070038795471191\n",
      "Epoch 3/25, Batch 28884/30044, Loss: 0.00955415889620781\n",
      "Epoch 3/25, Batch 29232/30044, Loss: 0.009509982541203499\n",
      "Epoch 3/25, Batch 29580/30044, Loss: 0.008575236424803734\n",
      "Epoch 3/25, Batch 29928/30044, Loss: 0.0093214837834239\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 28, '_step_count': 29, '_get_lr_called_within_step': False, '_last_lr': [3.5111757055874273e-06]}\n",
      "Epoch 4/25, Batch 0/30044, Loss: 0.009110627695918083\n",
      "Epoch 4/25, Batch 348/30044, Loss: 0.011031558737158775\n",
      "Epoch 4/25, Batch 696/30044, Loss: 0.008462577126920223\n",
      "Epoch 4/25, Batch 1044/30044, Loss: 0.007938142865896225\n",
      "Epoch 4/25, Batch 1392/30044, Loss: 0.009309973567724228\n",
      "Epoch 4/25, Batch 1740/30044, Loss: 0.009485912509262562\n",
      "Epoch 4/25, Batch 2088/30044, Loss: 0.010091871954500675\n",
      "Epoch 4/25, Batch 2436/30044, Loss: 0.008374768309295177\n",
      "Epoch 4/25, Batch 2784/30044, Loss: 0.010002439841628075\n",
      "Epoch 4/25, Batch 3132/30044, Loss: 0.009529579430818558\n",
      "Epoch 4/25, Batch 3480/30044, Loss: 0.009832107461988926\n",
      "Epoch 4/25, Batch 3828/30044, Loss: 0.009309476241469383\n",
      "Epoch 4/25, Batch 4176/30044, Loss: 0.010376570746302605\n",
      "Epoch 4/25, Batch 4524/30044, Loss: 0.008399279788136482\n",
      "Epoch 4/25, Batch 4872/30044, Loss: 0.00919055100530386\n",
      "Epoch 4/25, Batch 5220/30044, Loss: 0.011568380519747734\n",
      "Epoch 4/25, Batch 5568/30044, Loss: 0.010732256807386875\n",
      "Epoch 4/25, Batch 5916/30044, Loss: 0.008859867230057716\n",
      "Epoch 4/25, Batch 6264/30044, Loss: 0.009595755487680435\n",
      "Epoch 4/25, Batch 6612/30044, Loss: 0.009014833718538284\n",
      "Epoch 4/25, Batch 6960/30044, Loss: 0.011221091262996197\n",
      "Epoch 4/25, Batch 7308/30044, Loss: 0.008386334404349327\n",
      "Epoch 4/25, Batch 7656/30044, Loss: 0.010586682707071304\n",
      "Epoch 4/25, Batch 8004/30044, Loss: 0.009321369230747223\n",
      "Epoch 4/25, Batch 8352/30044, Loss: 0.0074887326918542385\n",
      "Epoch 4/25, Batch 8700/30044, Loss: 0.011191977187991142\n",
      "Epoch 4/25, Batch 9048/30044, Loss: 0.008697626180946827\n",
      "Epoch 4/25, Batch 9396/30044, Loss: 0.012105416506528854\n",
      "Epoch 4/25, Batch 9744/30044, Loss: 0.00968094076961279\n",
      "Epoch 4/25, Batch 10092/30044, Loss: 0.011464669369161129\n",
      "Epoch 4/25, Batch 10440/30044, Loss: 0.008495509624481201\n",
      "Epoch 4/25, Batch 10788/30044, Loss: 0.008954714983701706\n",
      "Epoch 4/25, Batch 11136/30044, Loss: 0.009405653923749924\n",
      "Epoch 4/25, Batch 11484/30044, Loss: 0.009870733134448528\n",
      "Epoch 4/25, Batch 11832/30044, Loss: 0.009709793142974377\n",
      "Epoch 4/25, Batch 12180/30044, Loss: 0.009671092964708805\n",
      "Epoch 4/25, Batch 12528/30044, Loss: 0.008766522631049156\n",
      "Epoch 4/25, Batch 12876/30044, Loss: 0.009242095984518528\n",
      "Epoch 4/25, Batch 13224/30044, Loss: 0.008434743620455265\n",
      "Epoch 4/25, Batch 13572/30044, Loss: 0.009692513383924961\n",
      "Epoch 4/25, Batch 13920/30044, Loss: 0.009553813375532627\n",
      "Epoch 4/25, Batch 14268/30044, Loss: 0.007836312055587769\n",
      "Epoch 4/25, Batch 14616/30044, Loss: 0.00927136093378067\n",
      "Epoch 4/25, Batch 14964/30044, Loss: 0.007558602374047041\n",
      "Epoch 4/25, Batch 15312/30044, Loss: 0.008848356083035469\n",
      "Epoch 4/25, Batch 15660/30044, Loss: 0.009114411659538746\n",
      "Epoch 4/25, Batch 16008/30044, Loss: 0.007952499203383923\n",
      "Epoch 4/25, Batch 16356/30044, Loss: 0.009894405491650105\n",
      "Epoch 4/25, Batch 16704/30044, Loss: 0.008468960411846638\n",
      "Epoch 4/25, Batch 17052/30044, Loss: 0.009622074663639069\n",
      "Epoch 4/25, Batch 17400/30044, Loss: 0.012868424877524376\n",
      "Epoch 4/25, Batch 17748/30044, Loss: 0.010369507595896721\n",
      "Epoch 4/25, Batch 18096/30044, Loss: 0.010290976613759995\n",
      "Epoch 4/25, Batch 18444/30044, Loss: 0.010491127148270607\n",
      "Epoch 4/25, Batch 18792/30044, Loss: 0.011591620743274689\n",
      "Epoch 4/25, Batch 19140/30044, Loss: 0.010185311548411846\n",
      "Epoch 4/25, Batch 19488/30044, Loss: 0.009412486106157303\n",
      "Epoch 4/25, Batch 19836/30044, Loss: 0.01026438269764185\n",
      "Epoch 4/25, Batch 20184/30044, Loss: 0.009171392768621445\n",
      "Epoch 4/25, Batch 20532/30044, Loss: 0.010130589827895164\n",
      "Epoch 4/25, Batch 20880/30044, Loss: 0.009127710945904255\n",
      "Epoch 4/25, Batch 21228/30044, Loss: 0.0076031871140003204\n",
      "Epoch 4/25, Batch 21576/30044, Loss: 0.010804754681885242\n",
      "Epoch 4/25, Batch 21924/30044, Loss: 0.009779573418200016\n",
      "Epoch 4/25, Batch 22272/30044, Loss: 0.009828182868659496\n",
      "Epoch 4/25, Batch 22620/30044, Loss: 0.009138205088675022\n",
      "Epoch 4/25, Batch 22968/30044, Loss: 0.010634636506438255\n",
      "Epoch 4/25, Batch 23316/30044, Loss: 0.009120984002947807\n",
      "Epoch 4/25, Batch 23664/30044, Loss: 0.00889590848237276\n",
      "Epoch 4/25, Batch 24012/30044, Loss: 0.011147900484502316\n",
      "Epoch 4/25, Batch 24360/30044, Loss: 0.01102977804839611\n",
      "Epoch 4/25, Batch 24708/30044, Loss: 0.008161420002579689\n",
      "Epoch 4/25, Batch 25056/30044, Loss: 0.010677337646484375\n",
      "Epoch 4/25, Batch 25404/30044, Loss: 0.009712964296340942\n",
      "Epoch 4/25, Batch 25752/30044, Loss: 0.009825772605836391\n",
      "Epoch 4/25, Batch 26100/30044, Loss: 0.010617285966873169\n",
      "Epoch 4/25, Batch 26448/30044, Loss: 0.00930742546916008\n",
      "Epoch 4/25, Batch 26796/30044, Loss: 0.009848088026046753\n",
      "Epoch 4/25, Batch 27144/30044, Loss: 0.009609553962945938\n",
      "Epoch 4/25, Batch 27492/30044, Loss: 0.009546928107738495\n",
      "Epoch 4/25, Batch 27840/30044, Loss: 0.00840438436716795\n",
      "Epoch 4/25, Batch 28188/30044, Loss: 0.008663746528327465\n",
      "Epoch 4/25, Batch 28536/30044, Loss: 0.010319079272449017\n",
      "Epoch 4/25, Batch 28884/30044, Loss: 0.011437896639108658\n",
      "Epoch 4/25, Batch 29232/30044, Loss: 0.01147106010466814\n",
      "Epoch 4/25, Batch 29580/30044, Loss: 0.008157568983733654\n",
      "Epoch 4/25, Batch 29928/30044, Loss: 0.00907868705689907\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 29, '_step_count': 30, '_get_lr_called_within_step': False, '_last_lr': [6.1846659978068265e-06]}\n",
      "Epoch 5/25, Batch 0/30044, Loss: 0.009988329373300076\n",
      "Epoch 5/25, Batch 348/30044, Loss: 0.01199713721871376\n",
      "Epoch 5/25, Batch 696/30044, Loss: 0.009810776449739933\n",
      "Epoch 5/25, Batch 1044/30044, Loss: 0.008974228985607624\n",
      "Epoch 5/25, Batch 1392/30044, Loss: 0.009211708791553974\n",
      "Epoch 5/25, Batch 1740/30044, Loss: 0.00933854840695858\n",
      "Epoch 5/25, Batch 2088/30044, Loss: 0.008887023665010929\n",
      "Epoch 5/25, Batch 2436/30044, Loss: 0.007839002646505833\n",
      "Epoch 5/25, Batch 2784/30044, Loss: 0.009000265039503574\n",
      "Epoch 5/25, Batch 3132/30044, Loss: 0.008711565285921097\n",
      "Epoch 5/25, Batch 3480/30044, Loss: 0.009691901504993439\n",
      "Epoch 5/25, Batch 3828/30044, Loss: 0.008487331680953503\n",
      "Epoch 5/25, Batch 4176/30044, Loss: 0.008268489502370358\n",
      "Epoch 5/25, Batch 4524/30044, Loss: 0.00857952143996954\n",
      "Epoch 5/25, Batch 4872/30044, Loss: 0.009282903745770454\n",
      "Epoch 5/25, Batch 5220/30044, Loss: 0.009581182152032852\n",
      "Epoch 5/25, Batch 5568/30044, Loss: 0.00869691837579012\n",
      "Epoch 5/25, Batch 5916/30044, Loss: 0.007402727380394936\n",
      "Epoch 5/25, Batch 6264/30044, Loss: 0.008884728886187077\n",
      "Epoch 5/25, Batch 6612/30044, Loss: 0.009047826752066612\n",
      "Epoch 5/25, Batch 6960/30044, Loss: 0.010048587806522846\n",
      "Epoch 5/25, Batch 7308/30044, Loss: 0.007975731045007706\n",
      "Epoch 5/25, Batch 7656/30044, Loss: 0.009265446104109287\n",
      "Epoch 5/25, Batch 8004/30044, Loss: 0.009505382739007473\n",
      "Epoch 5/25, Batch 8352/30044, Loss: 0.0075253271497786045\n",
      "Epoch 5/25, Batch 8700/30044, Loss: 0.008260512724518776\n",
      "Epoch 5/25, Batch 9048/30044, Loss: 0.009703071787953377\n",
      "Epoch 5/25, Batch 9396/30044, Loss: 0.009273756295442581\n",
      "Epoch 5/25, Batch 9744/30044, Loss: 0.009469184093177319\n",
      "Epoch 5/25, Batch 10092/30044, Loss: 0.009763609617948532\n",
      "Epoch 5/25, Batch 10440/30044, Loss: 0.009099922142922878\n",
      "Epoch 5/25, Batch 10788/30044, Loss: 0.008709119632840157\n",
      "Epoch 5/25, Batch 11136/30044, Loss: 0.009074831381440163\n",
      "Epoch 5/25, Batch 11484/30044, Loss: 0.009806315414607525\n",
      "Epoch 5/25, Batch 11832/30044, Loss: 0.009135069325566292\n",
      "Epoch 5/25, Batch 12180/30044, Loss: 0.008384771645069122\n",
      "Epoch 5/25, Batch 12528/30044, Loss: 0.007245037239044905\n",
      "Epoch 5/25, Batch 12876/30044, Loss: 0.00915179681032896\n",
      "Epoch 5/25, Batch 13224/30044, Loss: 0.009113985113799572\n",
      "Epoch 5/25, Batch 13572/30044, Loss: 0.010591374710202217\n",
      "Epoch 5/25, Batch 13920/30044, Loss: 0.008316676132380962\n",
      "Epoch 5/25, Batch 14268/30044, Loss: 0.009105604141950607\n",
      "Epoch 5/25, Batch 14616/30044, Loss: 0.009726709686219692\n",
      "Epoch 5/25, Batch 14964/30044, Loss: 0.01195757556706667\n",
      "Epoch 5/25, Batch 15312/30044, Loss: 0.011019028723239899\n",
      "Epoch 5/25, Batch 15660/30044, Loss: 0.01088429894298315\n",
      "Epoch 5/25, Batch 16008/30044, Loss: 0.008325636386871338\n",
      "Epoch 5/25, Batch 16356/30044, Loss: 0.009707300923764706\n",
      "Epoch 5/25, Batch 16704/30044, Loss: 0.009132078848779202\n",
      "Epoch 5/25, Batch 17052/30044, Loss: 0.0102959293872118\n",
      "Epoch 5/25, Batch 17400/30044, Loss: 0.007425399962812662\n",
      "Epoch 5/25, Batch 17748/30044, Loss: 0.009674531407654285\n",
      "Epoch 5/25, Batch 18096/30044, Loss: 0.00898334663361311\n",
      "Epoch 5/25, Batch 18444/30044, Loss: 0.009212834760546684\n",
      "Epoch 5/25, Batch 18792/30044, Loss: 0.010416544042527676\n",
      "Epoch 5/25, Batch 19140/30044, Loss: 0.010747958905994892\n",
      "Epoch 5/25, Batch 19488/30044, Loss: 0.010699686594307423\n",
      "Epoch 5/25, Batch 19836/30044, Loss: 0.008590334095060825\n",
      "Epoch 5/25, Batch 20184/30044, Loss: 0.009980658069252968\n",
      "Epoch 5/25, Batch 20532/30044, Loss: 0.007737691048532724\n",
      "Epoch 5/25, Batch 20880/30044, Loss: 0.008415157906711102\n",
      "Epoch 5/25, Batch 21228/30044, Loss: 0.010683830827474594\n",
      "Epoch 5/25, Batch 21576/30044, Loss: 0.01074281707406044\n",
      "Epoch 5/25, Batch 21924/30044, Loss: 0.00966317392885685\n",
      "Epoch 5/25, Batch 22272/30044, Loss: 0.008978979662060738\n",
      "Epoch 5/25, Batch 22620/30044, Loss: 0.00817176140844822\n",
      "Epoch 5/25, Batch 22968/30044, Loss: 0.008766774088144302\n",
      "Epoch 5/25, Batch 23316/30044, Loss: 0.010499885305762291\n",
      "Epoch 5/25, Batch 23664/30044, Loss: 0.008785891346633434\n",
      "Epoch 5/25, Batch 24012/30044, Loss: 0.008577115833759308\n",
      "Epoch 5/25, Batch 24360/30044, Loss: 0.0074694412760436535\n",
      "Epoch 5/25, Batch 24708/30044, Loss: 0.007939089089632034\n",
      "Epoch 5/25, Batch 25056/30044, Loss: 0.008436010219156742\n",
      "Epoch 5/25, Batch 25404/30044, Loss: 0.009247181937098503\n",
      "Epoch 5/25, Batch 25752/30044, Loss: 0.0099793067201972\n",
      "Epoch 5/25, Batch 26100/30044, Loss: 0.009015889838337898\n",
      "Epoch 5/25, Batch 26448/30044, Loss: 0.01038864254951477\n",
      "Epoch 5/25, Batch 26796/30044, Loss: 0.010198376141488552\n",
      "Epoch 5/25, Batch 27144/30044, Loss: 0.00871268380433321\n",
      "Epoch 5/25, Batch 27492/30044, Loss: 0.010391898453235626\n",
      "Epoch 5/25, Batch 27840/30044, Loss: 0.008128093555569649\n",
      "Epoch 5/25, Batch 28188/30044, Loss: 0.010025553405284882\n",
      "Epoch 5/25, Batch 28536/30044, Loss: 0.009903676807880402\n",
      "Epoch 5/25, Batch 28884/30044, Loss: 0.009227980859577656\n",
      "Epoch 5/25, Batch 29232/30044, Loss: 0.009254594333469868\n",
      "Epoch 5/25, Batch 29580/30044, Loss: 0.008070955984294415\n",
      "Epoch 5/25, Batch 29928/30044, Loss: 0.007567442487925291\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 30, '_step_count': 31, '_get_lr_called_within_step': False, '_last_lr': [9.549150281252611e-06]}\n",
      "Epoch 6/25, Batch 0/30044, Loss: 0.01090437825769186\n",
      "Epoch 6/25, Batch 348/30044, Loss: 0.008000741712749004\n",
      "Epoch 6/25, Batch 696/30044, Loss: 0.009091638028621674\n",
      "Epoch 6/25, Batch 1044/30044, Loss: 0.010851689614355564\n",
      "Epoch 6/25, Batch 1392/30044, Loss: 0.009287075139582157\n",
      "Epoch 6/25, Batch 1740/30044, Loss: 0.008743607439100742\n",
      "Epoch 6/25, Batch 2088/30044, Loss: 0.008265017531812191\n",
      "Epoch 6/25, Batch 2436/30044, Loss: 0.01065708976238966\n",
      "Epoch 6/25, Batch 2784/30044, Loss: 0.00963410921394825\n",
      "Epoch 6/25, Batch 3132/30044, Loss: 0.008343551307916641\n",
      "Epoch 6/25, Batch 3480/30044, Loss: 0.008858800865709782\n",
      "Epoch 6/25, Batch 3828/30044, Loss: 0.011587977409362793\n",
      "Epoch 6/25, Batch 4176/30044, Loss: 0.009573018178343773\n",
      "Epoch 6/25, Batch 4524/30044, Loss: 0.008507085032761097\n",
      "Epoch 6/25, Batch 4872/30044, Loss: 0.010941446758806705\n",
      "Epoch 6/25, Batch 5220/30044, Loss: 0.009280531667172909\n",
      "Epoch 6/25, Batch 5568/30044, Loss: 0.008499459363520145\n",
      "Epoch 6/25, Batch 5916/30044, Loss: 0.009946015663444996\n",
      "Epoch 6/25, Batch 6264/30044, Loss: 0.009133045561611652\n",
      "Epoch 6/25, Batch 6612/30044, Loss: 0.008260611444711685\n",
      "Epoch 6/25, Batch 6960/30044, Loss: 0.007880312390625477\n",
      "Epoch 6/25, Batch 7308/30044, Loss: 0.00924100261181593\n",
      "Epoch 6/25, Batch 7656/30044, Loss: 0.008000252768397331\n",
      "Epoch 6/25, Batch 8004/30044, Loss: 0.010015377774834633\n",
      "Epoch 6/25, Batch 8352/30044, Loss: 0.008607812225818634\n",
      "Epoch 6/25, Batch 8700/30044, Loss: 0.009076607413589954\n",
      "Epoch 6/25, Batch 9048/30044, Loss: 0.008728142827749252\n",
      "Epoch 6/25, Batch 9396/30044, Loss: 0.009169443510472775\n",
      "Epoch 6/25, Batch 9744/30044, Loss: 0.009695949964225292\n",
      "Epoch 6/25, Batch 10092/30044, Loss: 0.008212113752961159\n",
      "Epoch 6/25, Batch 10440/30044, Loss: 0.011246325448155403\n",
      "Epoch 6/25, Batch 10788/30044, Loss: 0.011539094150066376\n",
      "Epoch 6/25, Batch 11136/30044, Loss: 0.010157819837331772\n",
      "Epoch 6/25, Batch 11484/30044, Loss: 0.006741478107869625\n",
      "Epoch 6/25, Batch 11832/30044, Loss: 0.010154183022677898\n",
      "Epoch 6/25, Batch 12180/30044, Loss: 0.010934218764305115\n",
      "Epoch 6/25, Batch 12528/30044, Loss: 0.011333120986819267\n",
      "Epoch 6/25, Batch 12876/30044, Loss: 0.008300935849547386\n",
      "Epoch 6/25, Batch 13224/30044, Loss: 0.009445291012525558\n",
      "Epoch 6/25, Batch 13572/30044, Loss: 0.010352252051234245\n",
      "Epoch 6/25, Batch 13920/30044, Loss: 0.009033601731061935\n",
      "Epoch 6/25, Batch 14268/30044, Loss: 0.009279564023017883\n",
      "Epoch 6/25, Batch 14616/30044, Loss: 0.008904253132641315\n",
      "Epoch 6/25, Batch 14964/30044, Loss: 0.010245878249406815\n",
      "Epoch 6/25, Batch 15312/30044, Loss: 0.009929822757840157\n",
      "Epoch 6/25, Batch 15660/30044, Loss: 0.008866975083947182\n",
      "Epoch 6/25, Batch 16008/30044, Loss: 0.010965823195874691\n",
      "Epoch 6/25, Batch 16356/30044, Loss: 0.009813494049012661\n",
      "Epoch 6/25, Batch 16704/30044, Loss: 0.008671672083437443\n",
      "Epoch 6/25, Batch 17052/30044, Loss: 0.00814204290509224\n",
      "Epoch 6/25, Batch 17400/30044, Loss: 0.010317315347492695\n",
      "Epoch 6/25, Batch 17748/30044, Loss: 0.010812114924192429\n",
      "Epoch 6/25, Batch 18096/30044, Loss: 0.009562131017446518\n",
      "Epoch 6/25, Batch 18444/30044, Loss: 0.008685841225087643\n",
      "Epoch 6/25, Batch 18792/30044, Loss: 0.008352821692824364\n",
      "Epoch 6/25, Batch 19140/30044, Loss: 0.00982368178665638\n",
      "Epoch 6/25, Batch 19488/30044, Loss: 0.01027990784496069\n",
      "Epoch 6/25, Batch 19836/30044, Loss: 0.007437535561621189\n",
      "Epoch 6/25, Batch 20184/30044, Loss: 0.008910131640732288\n",
      "Epoch 6/25, Batch 20532/30044, Loss: 0.010422193445265293\n",
      "Epoch 6/25, Batch 20880/30044, Loss: 0.008536064065992832\n",
      "Epoch 6/25, Batch 21228/30044, Loss: 0.009558905847370625\n",
      "Epoch 6/25, Batch 21576/30044, Loss: 0.010203122161328793\n",
      "Epoch 6/25, Batch 21924/30044, Loss: 0.00793309323489666\n",
      "Epoch 6/25, Batch 22272/30044, Loss: 0.008866882883012295\n",
      "Epoch 6/25, Batch 22620/30044, Loss: 0.011510264128446579\n",
      "Epoch 6/25, Batch 22968/30044, Loss: 0.007918823510408401\n",
      "Epoch 6/25, Batch 23316/30044, Loss: 0.009911801666021347\n",
      "Epoch 6/25, Batch 23664/30044, Loss: 0.00984378345310688\n",
      "Epoch 6/25, Batch 24012/30044, Loss: 0.010348142124712467\n",
      "Epoch 6/25, Batch 24360/30044, Loss: 0.010413245297968388\n",
      "Epoch 6/25, Batch 24708/30044, Loss: 0.008792233653366566\n",
      "Epoch 6/25, Batch 25056/30044, Loss: 0.008063697256147861\n",
      "Epoch 6/25, Batch 25404/30044, Loss: 0.00817897543311119\n",
      "Epoch 6/25, Batch 25752/30044, Loss: 0.01011327002197504\n",
      "Epoch 6/25, Batch 26100/30044, Loss: 0.008931167423725128\n",
      "Epoch 6/25, Batch 26448/30044, Loss: 0.009138562716543674\n",
      "Epoch 6/25, Batch 26796/30044, Loss: 0.01055102702230215\n",
      "Epoch 6/25, Batch 27144/30044, Loss: 0.009916801936924458\n",
      "Epoch 6/25, Batch 27492/30044, Loss: 0.010519926436245441\n",
      "Epoch 6/25, Batch 27840/30044, Loss: 0.011698574759066105\n",
      "Epoch 6/25, Batch 28188/30044, Loss: 0.009373880922794342\n",
      "Epoch 6/25, Batch 28536/30044, Loss: 0.007493344135582447\n",
      "Epoch 6/25, Batch 28884/30044, Loss: 0.008790593594312668\n",
      "Epoch 6/25, Batch 29232/30044, Loss: 0.00858526024967432\n",
      "Epoch 6/25, Batch 29580/30044, Loss: 0.009617112576961517\n",
      "Epoch 6/25, Batch 29928/30044, Loss: 0.007176727522164583\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 31, '_step_count': 32, '_get_lr_called_within_step': False, '_last_lr': [1.3551568628929405e-05]}\n",
      "Epoch 7/25, Batch 0/30044, Loss: 0.007752618752419949\n",
      "Epoch 7/25, Batch 348/30044, Loss: 0.010130266658961773\n",
      "Epoch 7/25, Batch 696/30044, Loss: 0.009056231006979942\n",
      "Epoch 7/25, Batch 1044/30044, Loss: 0.00884577352553606\n",
      "Epoch 7/25, Batch 1392/30044, Loss: 0.0076735904440283775\n",
      "Epoch 7/25, Batch 1740/30044, Loss: 0.007905114442110062\n",
      "Epoch 7/25, Batch 2088/30044, Loss: 0.009035014547407627\n",
      "Epoch 7/25, Batch 2436/30044, Loss: 0.008961590938270092\n",
      "Epoch 7/25, Batch 2784/30044, Loss: 0.009180520661175251\n",
      "Epoch 7/25, Batch 3132/30044, Loss: 0.009668083861470222\n",
      "Epoch 7/25, Batch 3480/30044, Loss: 0.007455936633050442\n",
      "Epoch 7/25, Batch 3828/30044, Loss: 0.00798910390585661\n",
      "Epoch 7/25, Batch 4176/30044, Loss: 0.008775105699896812\n",
      "Epoch 7/25, Batch 4524/30044, Loss: 0.006932297255843878\n",
      "Epoch 7/25, Batch 4872/30044, Loss: 0.00834751408547163\n",
      "Epoch 7/25, Batch 5220/30044, Loss: 0.010697823949158192\n",
      "Epoch 7/25, Batch 5568/30044, Loss: 0.009978082031011581\n",
      "Epoch 7/25, Batch 5916/30044, Loss: 0.007828911766409874\n",
      "Epoch 7/25, Batch 6264/30044, Loss: 0.010065492242574692\n",
      "Epoch 7/25, Batch 6612/30044, Loss: 0.009554996155202389\n",
      "Epoch 7/25, Batch 6960/30044, Loss: 0.009246399626135826\n",
      "Epoch 7/25, Batch 7308/30044, Loss: 0.008341476321220398\n",
      "Epoch 7/25, Batch 7656/30044, Loss: 0.008923967368900776\n",
      "Epoch 7/25, Batch 8004/30044, Loss: 0.010435634292662144\n",
      "Epoch 7/25, Batch 8352/30044, Loss: 0.009149441495537758\n",
      "Epoch 7/25, Batch 8700/30044, Loss: 0.009666331112384796\n",
      "Epoch 7/25, Batch 9048/30044, Loss: 0.008735218085348606\n",
      "Epoch 7/25, Batch 9396/30044, Loss: 0.009577075019478798\n",
      "Epoch 7/25, Batch 9744/30044, Loss: 0.009426054544746876\n",
      "Epoch 7/25, Batch 10092/30044, Loss: 0.007669669575989246\n",
      "Epoch 7/25, Batch 10440/30044, Loss: 0.00871228240430355\n",
      "Epoch 7/25, Batch 10788/30044, Loss: 0.00721993250772357\n",
      "Epoch 7/25, Batch 11136/30044, Loss: 0.008557166904211044\n",
      "Epoch 7/25, Batch 11484/30044, Loss: 0.009532646276056767\n",
      "Epoch 7/25, Batch 11832/30044, Loss: 0.010377579368650913\n",
      "Epoch 7/25, Batch 12180/30044, Loss: 0.009316062554717064\n",
      "Epoch 7/25, Batch 12528/30044, Loss: 0.009811126627027988\n",
      "Epoch 7/25, Batch 12876/30044, Loss: 0.009393621236085892\n",
      "Epoch 7/25, Batch 13224/30044, Loss: 0.007809655740857124\n",
      "Epoch 7/25, Batch 13572/30044, Loss: 0.00782776903361082\n",
      "Epoch 7/25, Batch 13920/30044, Loss: 0.008261799812316895\n",
      "Epoch 7/25, Batch 14268/30044, Loss: 0.00890741404145956\n",
      "Epoch 7/25, Batch 14616/30044, Loss: 0.010001060552895069\n",
      "Epoch 7/25, Batch 14964/30044, Loss: 0.007807077374309301\n",
      "Epoch 7/25, Batch 15312/30044, Loss: 0.007213997654616833\n",
      "Epoch 7/25, Batch 15660/30044, Loss: 0.00938736367970705\n",
      "Epoch 7/25, Batch 16008/30044, Loss: 0.008491207845509052\n",
      "Epoch 7/25, Batch 16356/30044, Loss: 0.00908245425671339\n",
      "Epoch 7/25, Batch 16704/30044, Loss: 0.010445800609886646\n",
      "Epoch 7/25, Batch 17052/30044, Loss: 0.0110344672575593\n",
      "Epoch 7/25, Batch 17400/30044, Loss: 0.010304469615221024\n",
      "Epoch 7/25, Batch 17748/30044, Loss: 0.008818124420940876\n",
      "Epoch 7/25, Batch 18096/30044, Loss: 0.008988513611257076\n",
      "Epoch 7/25, Batch 18444/30044, Loss: 0.009264404885470867\n",
      "Epoch 7/25, Batch 18792/30044, Loss: 0.008714859373867512\n",
      "Epoch 7/25, Batch 19140/30044, Loss: 0.0087946942076087\n",
      "Epoch 7/25, Batch 19488/30044, Loss: 0.00934829656034708\n",
      "Epoch 7/25, Batch 19836/30044, Loss: 0.010826379992067814\n",
      "Epoch 7/25, Batch 20184/30044, Loss: 0.009365575388073921\n",
      "Epoch 7/25, Batch 20532/30044, Loss: 0.008897965773940086\n",
      "Epoch 7/25, Batch 20880/30044, Loss: 0.009020993486046791\n",
      "Epoch 7/25, Batch 21228/30044, Loss: 0.009547911584377289\n",
      "Epoch 7/25, Batch 21576/30044, Loss: 0.0094135832041502\n",
      "Epoch 7/25, Batch 21924/30044, Loss: 0.009780910797417164\n",
      "Epoch 7/25, Batch 22272/30044, Loss: 0.01002197340130806\n",
      "Epoch 7/25, Batch 22620/30044, Loss: 0.009595292620360851\n",
      "Epoch 7/25, Batch 22968/30044, Loss: 0.009052290581166744\n",
      "Epoch 7/25, Batch 23316/30044, Loss: 0.008997232653200626\n",
      "Epoch 7/25, Batch 23664/30044, Loss: 0.010103360749781132\n",
      "Epoch 7/25, Batch 24012/30044, Loss: 0.0077614025212824345\n",
      "Epoch 7/25, Batch 24360/30044, Loss: 0.009225843474268913\n",
      "Epoch 7/25, Batch 24708/30044, Loss: 0.008412690833210945\n",
      "Epoch 7/25, Batch 25056/30044, Loss: 0.009220167063176632\n",
      "Epoch 7/25, Batch 25404/30044, Loss: 0.009684627875685692\n",
      "Epoch 7/25, Batch 25752/30044, Loss: 0.010945227928459644\n",
      "Epoch 7/25, Batch 26100/30044, Loss: 0.010456612333655357\n",
      "Epoch 7/25, Batch 26448/30044, Loss: 0.010176524519920349\n",
      "Epoch 7/25, Batch 26796/30044, Loss: 0.009962189942598343\n",
      "Epoch 7/25, Batch 27144/30044, Loss: 0.012049171142280102\n",
      "Epoch 7/25, Batch 27492/30044, Loss: 0.00949010532349348\n",
      "Epoch 7/25, Batch 27840/30044, Loss: 0.00785050168633461\n",
      "Epoch 7/25, Batch 28188/30044, Loss: 0.009248682297766209\n",
      "Epoch 7/25, Batch 28536/30044, Loss: 0.009226959198713303\n",
      "Epoch 7/25, Batch 28884/30044, Loss: 0.00854955893009901\n",
      "Epoch 7/25, Batch 29232/30044, Loss: 0.008799090050160885\n",
      "Epoch 7/25, Batch 29580/30044, Loss: 0.007829018868505955\n",
      "Epoch 7/25, Batch 29928/30044, Loss: 0.009732699021697044\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 32, '_step_count': 33, '_get_lr_called_within_step': False, '_last_lr': [1.8128800512565523e-05]}\n",
      "Epoch 8/25, Batch 0/30044, Loss: 0.00821189396083355\n",
      "Epoch 8/25, Batch 348/30044, Loss: 0.008224469609558582\n",
      "Epoch 8/25, Batch 696/30044, Loss: 0.00926692970097065\n",
      "Epoch 8/25, Batch 1044/30044, Loss: 0.0084462845697999\n",
      "Epoch 8/25, Batch 1392/30044, Loss: 0.008812828920781612\n",
      "Epoch 8/25, Batch 1740/30044, Loss: 0.008380944840610027\n",
      "Epoch 8/25, Batch 2088/30044, Loss: 0.010566940531134605\n",
      "Epoch 8/25, Batch 2436/30044, Loss: 0.009835991077125072\n",
      "Epoch 8/25, Batch 2784/30044, Loss: 0.010401763953268528\n",
      "Epoch 8/25, Batch 3132/30044, Loss: 0.010086500085890293\n",
      "Epoch 8/25, Batch 3480/30044, Loss: 0.008911349810659885\n",
      "Epoch 8/25, Batch 3828/30044, Loss: 0.01042010635137558\n",
      "Epoch 8/25, Batch 4176/30044, Loss: 0.010573308914899826\n",
      "Epoch 8/25, Batch 4524/30044, Loss: 0.008185936138033867\n",
      "Epoch 8/25, Batch 4872/30044, Loss: 0.01096588745713234\n",
      "Epoch 8/25, Batch 5220/30044, Loss: 0.008672804571688175\n",
      "Epoch 8/25, Batch 5568/30044, Loss: 0.009971336461603642\n",
      "Epoch 8/25, Batch 5916/30044, Loss: 0.00796713400632143\n",
      "Epoch 8/25, Batch 6264/30044, Loss: 0.01129927858710289\n",
      "Epoch 8/25, Batch 6612/30044, Loss: 0.009262099862098694\n",
      "Epoch 8/25, Batch 6960/30044, Loss: 0.009351855143904686\n",
      "Epoch 8/25, Batch 7308/30044, Loss: 0.00867735967040062\n",
      "Epoch 8/25, Batch 7656/30044, Loss: 0.007986658252775669\n",
      "Epoch 8/25, Batch 8004/30044, Loss: 0.007846702821552753\n",
      "Epoch 8/25, Batch 8352/30044, Loss: 0.007399501744657755\n",
      "Epoch 8/25, Batch 8700/30044, Loss: 0.009528294205665588\n",
      "Epoch 8/25, Batch 9048/30044, Loss: 0.008914893493056297\n",
      "Epoch 8/25, Batch 9396/30044, Loss: 0.009540959261357784\n",
      "Epoch 8/25, Batch 9744/30044, Loss: 0.007668436039239168\n",
      "Epoch 8/25, Batch 10092/30044, Loss: 0.007186646573245525\n",
      "Epoch 8/25, Batch 10440/30044, Loss: 0.008726261556148529\n",
      "Epoch 8/25, Batch 10788/30044, Loss: 0.00953279435634613\n",
      "Epoch 8/25, Batch 11136/30044, Loss: 0.011520816944539547\n",
      "Epoch 8/25, Batch 11484/30044, Loss: 0.008593732491135597\n",
      "Epoch 8/25, Batch 11832/30044, Loss: 0.009140562266111374\n",
      "Epoch 8/25, Batch 12180/30044, Loss: 0.009545705281198025\n",
      "Epoch 8/25, Batch 12528/30044, Loss: 0.011265222914516926\n",
      "Epoch 8/25, Batch 12876/30044, Loss: 0.008420072495937347\n",
      "Epoch 8/25, Batch 13224/30044, Loss: 0.011932309716939926\n",
      "Epoch 8/25, Batch 13572/30044, Loss: 0.010052657686173916\n",
      "Epoch 8/25, Batch 13920/30044, Loss: 0.010488905943930149\n",
      "Epoch 8/25, Batch 14268/30044, Loss: 0.007948270067572594\n",
      "Epoch 8/25, Batch 14616/30044, Loss: 0.008774555288255215\n",
      "Epoch 8/25, Batch 14964/30044, Loss: 0.008191959001123905\n",
      "Epoch 8/25, Batch 15312/30044, Loss: 0.008843985386192799\n",
      "Epoch 8/25, Batch 15660/30044, Loss: 0.009714365936815739\n",
      "Epoch 8/25, Batch 16008/30044, Loss: 0.00880757998675108\n",
      "Epoch 8/25, Batch 16356/30044, Loss: 0.00959640834480524\n",
      "Epoch 8/25, Batch 16704/30044, Loss: 0.00981133058667183\n",
      "Epoch 8/25, Batch 17052/30044, Loss: 0.009311011992394924\n",
      "Epoch 8/25, Batch 17400/30044, Loss: 0.009192099794745445\n",
      "Epoch 8/25, Batch 17748/30044, Loss: 0.009914660826325417\n",
      "Epoch 8/25, Batch 18096/30044, Loss: 0.011095530353486538\n",
      "Epoch 8/25, Batch 18444/30044, Loss: 0.008646050468087196\n",
      "Epoch 8/25, Batch 18792/30044, Loss: 0.008943065069615841\n",
      "Epoch 8/25, Batch 19140/30044, Loss: 0.008223316632211208\n",
      "Epoch 8/25, Batch 19488/30044, Loss: 0.008023673668503761\n",
      "Epoch 8/25, Batch 19836/30044, Loss: 0.009361089207231998\n",
      "Epoch 8/25, Batch 20184/30044, Loss: 0.009184707887470722\n",
      "Epoch 8/25, Batch 20532/30044, Loss: 0.010768908075988293\n",
      "Epoch 8/25, Batch 20880/30044, Loss: 0.010069114156067371\n",
      "Epoch 8/25, Batch 21228/30044, Loss: 0.00882053654640913\n",
      "Epoch 8/25, Batch 21576/30044, Loss: 0.008964335545897484\n",
      "Epoch 8/25, Batch 21924/30044, Loss: 0.009892098605632782\n",
      "Epoch 8/25, Batch 22272/30044, Loss: 0.009124833159148693\n",
      "Epoch 8/25, Batch 22620/30044, Loss: 0.010424279607832432\n",
      "Epoch 8/25, Batch 22968/30044, Loss: 0.01072018127888441\n",
      "Epoch 8/25, Batch 23316/30044, Loss: 0.009161924012005329\n",
      "Epoch 8/25, Batch 23664/30044, Loss: 0.008426246233284473\n",
      "Epoch 8/25, Batch 24012/30044, Loss: 0.008888398297131062\n",
      "Epoch 8/25, Batch 24360/30044, Loss: 0.00806746818125248\n",
      "Epoch 8/25, Batch 24708/30044, Loss: 0.009217727929353714\n",
      "Epoch 8/25, Batch 25056/30044, Loss: 0.007769050542265177\n",
      "Epoch 8/25, Batch 25404/30044, Loss: 0.010472506284713745\n",
      "Epoch 8/25, Batch 25752/30044, Loss: 0.010328545235097408\n",
      "Epoch 8/25, Batch 26100/30044, Loss: 0.009928833693265915\n",
      "Epoch 8/25, Batch 26448/30044, Loss: 0.011265292763710022\n",
      "Epoch 8/25, Batch 26796/30044, Loss: 0.00948886014521122\n",
      "Epoch 8/25, Batch 27144/30044, Loss: 0.008618593215942383\n",
      "Epoch 8/25, Batch 27492/30044, Loss: 0.008892694488167763\n",
      "Epoch 8/25, Batch 27840/30044, Loss: 0.0077506545931100845\n",
      "Epoch 8/25, Batch 28188/30044, Loss: 0.008371960371732712\n",
      "Epoch 8/25, Batch 28536/30044, Loss: 0.01027708314359188\n",
      "Epoch 8/25, Batch 28884/30044, Loss: 0.010577681474387646\n",
      "Epoch 8/25, Batch 29232/30044, Loss: 0.009149730205535889\n",
      "Epoch 8/25, Batch 29580/30044, Loss: 0.009676344692707062\n",
      "Epoch 8/25, Batch 29928/30044, Loss: 0.007227610796689987\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 33, '_step_count': 34, '_get_lr_called_within_step': False, '_last_lr': [2.3208660251050182e-05]}\n",
      "Epoch 9/25, Batch 0/30044, Loss: 0.00881215650588274\n",
      "Epoch 9/25, Batch 348/30044, Loss: 0.009037192910909653\n",
      "Epoch 9/25, Batch 696/30044, Loss: 0.007529560476541519\n",
      "Epoch 9/25, Batch 1044/30044, Loss: 0.009811212308704853\n",
      "Epoch 9/25, Batch 1392/30044, Loss: 0.007875997573137283\n",
      "Epoch 9/25, Batch 1740/30044, Loss: 0.009178201667964458\n",
      "Epoch 9/25, Batch 2088/30044, Loss: 0.009157712571322918\n",
      "Epoch 9/25, Batch 2436/30044, Loss: 0.012311973609030247\n",
      "Epoch 9/25, Batch 2784/30044, Loss: 0.009102605283260345\n",
      "Epoch 9/25, Batch 3132/30044, Loss: 0.009957361966371536\n",
      "Epoch 9/25, Batch 3480/30044, Loss: 0.007863505743443966\n",
      "Epoch 9/25, Batch 3828/30044, Loss: 0.00957825779914856\n",
      "Epoch 9/25, Batch 4176/30044, Loss: 0.008283862844109535\n",
      "Epoch 9/25, Batch 4524/30044, Loss: 0.008993230760097504\n",
      "Epoch 9/25, Batch 4872/30044, Loss: 0.008671179413795471\n",
      "Epoch 9/25, Batch 5220/30044, Loss: 0.010206779465079308\n",
      "Epoch 9/25, Batch 5568/30044, Loss: 0.008780906908214092\n",
      "Epoch 9/25, Batch 5916/30044, Loss: 0.009035474620759487\n",
      "Epoch 9/25, Batch 6264/30044, Loss: 0.010136280208826065\n",
      "Epoch 9/25, Batch 6612/30044, Loss: 0.00829458050429821\n",
      "Epoch 9/25, Batch 6960/30044, Loss: 0.008082575164735317\n",
      "Epoch 9/25, Batch 7308/30044, Loss: 0.008707363158464432\n",
      "Epoch 9/25, Batch 7656/30044, Loss: 0.008712450042366982\n",
      "Epoch 9/25, Batch 8004/30044, Loss: 0.008809393271803856\n",
      "Epoch 9/25, Batch 8352/30044, Loss: 0.009930018335580826\n",
      "Epoch 9/25, Batch 8700/30044, Loss: 0.009753079153597355\n",
      "Epoch 9/25, Batch 9048/30044, Loss: 0.009934675879776478\n",
      "Epoch 9/25, Batch 9396/30044, Loss: 0.010478388518095016\n",
      "Epoch 9/25, Batch 9744/30044, Loss: 0.009255662560462952\n",
      "Epoch 9/25, Batch 10092/30044, Loss: 0.0086639653891325\n",
      "Epoch 9/25, Batch 10440/30044, Loss: 0.009216039441525936\n",
      "Epoch 9/25, Batch 10788/30044, Loss: 0.010299868881702423\n",
      "Epoch 9/25, Batch 11136/30044, Loss: 0.008364028297364712\n",
      "Epoch 9/25, Batch 11484/30044, Loss: 0.009091883897781372\n",
      "Epoch 9/25, Batch 11832/30044, Loss: 0.009898520074784756\n",
      "Epoch 9/25, Batch 12180/30044, Loss: 0.009326320141553879\n",
      "Epoch 9/25, Batch 12528/30044, Loss: 0.011188419535756111\n",
      "Epoch 9/25, Batch 12876/30044, Loss: 0.008088055066764355\n",
      "Epoch 9/25, Batch 13224/30044, Loss: 0.010086553171277046\n",
      "Epoch 9/25, Batch 13572/30044, Loss: 0.008734149858355522\n",
      "Epoch 9/25, Batch 13920/30044, Loss: 0.010739821009337902\n",
      "Epoch 9/25, Batch 14268/30044, Loss: 0.008372231386601925\n",
      "Epoch 9/25, Batch 14616/30044, Loss: 0.00869891420006752\n",
      "Epoch 9/25, Batch 14964/30044, Loss: 0.009596322663128376\n",
      "Epoch 9/25, Batch 15312/30044, Loss: 0.010618813335895538\n",
      "Epoch 9/25, Batch 15660/30044, Loss: 0.01020884420722723\n",
      "Epoch 9/25, Batch 16008/30044, Loss: 0.009082336910068989\n",
      "Epoch 9/25, Batch 16356/30044, Loss: 0.008061799220740795\n",
      "Epoch 9/25, Batch 16704/30044, Loss: 0.008907799609005451\n",
      "Epoch 9/25, Batch 17052/30044, Loss: 0.009541730396449566\n",
      "Epoch 9/25, Batch 17400/30044, Loss: 0.008371163159608841\n",
      "Epoch 9/25, Batch 17748/30044, Loss: 0.008083933964371681\n",
      "Epoch 9/25, Batch 18096/30044, Loss: 0.008798783645033836\n",
      "Epoch 9/25, Batch 18444/30044, Loss: 0.008990222588181496\n",
      "Epoch 9/25, Batch 18792/30044, Loss: 0.008259527385234833\n",
      "Epoch 9/25, Batch 19140/30044, Loss: 0.00881129689514637\n",
      "Epoch 9/25, Batch 19488/30044, Loss: 0.011799675412476063\n",
      "Epoch 9/25, Batch 19836/30044, Loss: 0.00863545574247837\n",
      "Epoch 9/25, Batch 20184/30044, Loss: 0.007953020744025707\n",
      "Epoch 9/25, Batch 20532/30044, Loss: 0.00818328931927681\n",
      "Epoch 9/25, Batch 20880/30044, Loss: 0.007529167458415031\n",
      "Epoch 9/25, Batch 21228/30044, Loss: 0.009697291068732738\n",
      "Epoch 9/25, Batch 21576/30044, Loss: 0.008359987288713455\n",
      "Epoch 9/25, Batch 21924/30044, Loss: 0.009118837304413319\n",
      "Epoch 9/25, Batch 22272/30044, Loss: 0.008235357701778412\n",
      "Epoch 9/25, Batch 22620/30044, Loss: 0.009200586006045341\n",
      "Epoch 9/25, Batch 22968/30044, Loss: 0.009968694299459457\n",
      "Epoch 9/25, Batch 23316/30044, Loss: 0.009081793949007988\n",
      "Epoch 9/25, Batch 23664/30044, Loss: 0.011542106047272682\n",
      "Epoch 9/25, Batch 24012/30044, Loss: 0.012443367391824722\n",
      "Epoch 9/25, Batch 24360/30044, Loss: 0.008251913823187351\n",
      "Epoch 9/25, Batch 24708/30044, Loss: 0.011071606539189816\n",
      "Epoch 9/25, Batch 25056/30044, Loss: 0.009764059446752071\n",
      "Epoch 9/25, Batch 25404/30044, Loss: 0.00813318882137537\n",
      "Epoch 9/25, Batch 25752/30044, Loss: 0.010105174966156483\n",
      "Epoch 9/25, Batch 26100/30044, Loss: 0.008232522755861282\n",
      "Epoch 9/25, Batch 26448/30044, Loss: 0.008070583455264568\n",
      "Epoch 9/25, Batch 26796/30044, Loss: 0.011428463272750378\n",
      "Epoch 9/25, Batch 27144/30044, Loss: 0.008933058008551598\n",
      "Epoch 9/25, Batch 27492/30044, Loss: 0.009109394624829292\n",
      "Epoch 9/25, Batch 27840/30044, Loss: 0.008769556879997253\n",
      "Epoch 9/25, Batch 28188/30044, Loss: 0.008135034702718258\n",
      "Epoch 9/25, Batch 28536/30044, Loss: 0.010552827268838882\n",
      "Epoch 9/25, Batch 28884/30044, Loss: 0.009100052528083324\n",
      "Epoch 9/25, Batch 29232/30044, Loss: 0.009137374348938465\n",
      "Epoch 9/25, Batch 29580/30044, Loss: 0.012614249251782894\n",
      "Epoch 9/25, Batch 29928/30044, Loss: 0.009695600718259811\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 34, '_step_count': 35, '_get_lr_called_within_step': False, '_last_lr': [2.8711035421746387e-05]}\n",
      "Epoch 10/25, Batch 0/30044, Loss: 0.009214134886860847\n",
      "Epoch 10/25, Batch 348/30044, Loss: 0.009832915849983692\n",
      "Epoch 10/25, Batch 696/30044, Loss: 0.011139290407299995\n",
      "Epoch 10/25, Batch 1044/30044, Loss: 0.010387800633907318\n",
      "Epoch 10/25, Batch 1392/30044, Loss: 0.010489614680409431\n",
      "Epoch 10/25, Batch 1740/30044, Loss: 0.009879705496132374\n",
      "Epoch 10/25, Batch 2088/30044, Loss: 0.009338353760540485\n",
      "Epoch 10/25, Batch 2436/30044, Loss: 0.009445658884942532\n",
      "Epoch 10/25, Batch 2784/30044, Loss: 0.0077221812680363655\n",
      "Epoch 10/25, Batch 3132/30044, Loss: 0.00908918958157301\n",
      "Epoch 10/25, Batch 3480/30044, Loss: 0.009738372638821602\n",
      "Epoch 10/25, Batch 3828/30044, Loss: 0.00974670983850956\n",
      "Epoch 10/25, Batch 4176/30044, Loss: 0.007814396172761917\n",
      "Epoch 10/25, Batch 4524/30044, Loss: 0.009120852686464787\n",
      "Epoch 10/25, Batch 4872/30044, Loss: 0.007996277883648872\n",
      "Epoch 10/25, Batch 5220/30044, Loss: 0.009186772629618645\n",
      "Epoch 10/25, Batch 5568/30044, Loss: 0.009231171570718288\n",
      "Epoch 10/25, Batch 5916/30044, Loss: 0.009798821993172169\n",
      "Epoch 10/25, Batch 6264/30044, Loss: 0.009339841082692146\n",
      "Epoch 10/25, Batch 6612/30044, Loss: 0.009027662687003613\n",
      "Epoch 10/25, Batch 6960/30044, Loss: 0.009679238311946392\n",
      "Epoch 10/25, Batch 7308/30044, Loss: 0.009733470156788826\n",
      "Epoch 10/25, Batch 7656/30044, Loss: 0.010854058898985386\n",
      "Epoch 10/25, Batch 8004/30044, Loss: 0.00904211774468422\n",
      "Epoch 10/25, Batch 8352/30044, Loss: 0.008857632987201214\n",
      "Epoch 10/25, Batch 8700/30044, Loss: 0.009485773742198944\n",
      "Epoch 10/25, Batch 9048/30044, Loss: 0.009982510469853878\n",
      "Epoch 10/25, Batch 9396/30044, Loss: 0.009232447482645512\n",
      "Epoch 10/25, Batch 9744/30044, Loss: 0.010426286607980728\n",
      "Epoch 10/25, Batch 10092/30044, Loss: 0.01063641905784607\n",
      "Epoch 10/25, Batch 10440/30044, Loss: 0.008962692692875862\n",
      "Epoch 10/25, Batch 10788/30044, Loss: 0.009423963725566864\n",
      "Epoch 10/25, Batch 11136/30044, Loss: 0.007750379387289286\n",
      "Epoch 10/25, Batch 11484/30044, Loss: 0.011222662404179573\n",
      "Epoch 10/25, Batch 11832/30044, Loss: 0.007471147924661636\n",
      "Epoch 10/25, Batch 12180/30044, Loss: 0.00896315649151802\n",
      "Epoch 10/25, Batch 12528/30044, Loss: 0.008396574296057224\n",
      "Epoch 10/25, Batch 12876/30044, Loss: 0.008299978449940681\n",
      "Epoch 10/25, Batch 13224/30044, Loss: 0.009040502831339836\n",
      "Epoch 10/25, Batch 13572/30044, Loss: 0.011355872265994549\n",
      "Epoch 10/25, Batch 13920/30044, Loss: 0.008919992484152317\n",
      "Epoch 10/25, Batch 14268/30044, Loss: 0.00841442495584488\n",
      "Epoch 10/25, Batch 14616/30044, Loss: 0.009619396179914474\n",
      "Epoch 10/25, Batch 14964/30044, Loss: 0.009060029871761799\n",
      "Epoch 10/25, Batch 15312/30044, Loss: 0.009360000491142273\n",
      "Epoch 10/25, Batch 15660/30044, Loss: 0.009757401421666145\n",
      "Epoch 10/25, Batch 16008/30044, Loss: 0.010980559512972832\n",
      "Epoch 10/25, Batch 16356/30044, Loss: 0.007680972572416067\n",
      "Epoch 10/25, Batch 16704/30044, Loss: 0.00974993035197258\n",
      "Epoch 10/25, Batch 17052/30044, Loss: 0.010233216919004917\n",
      "Epoch 10/25, Batch 17400/30044, Loss: 0.007817431353032589\n",
      "Epoch 10/25, Batch 17748/30044, Loss: 0.009861933067440987\n",
      "Epoch 10/25, Batch 18096/30044, Loss: 0.008778938092291355\n",
      "Epoch 10/25, Batch 18444/30044, Loss: 0.00948347244411707\n",
      "Epoch 10/25, Batch 18792/30044, Loss: 0.008479174226522446\n",
      "Epoch 10/25, Batch 19140/30044, Loss: 0.009080704301595688\n",
      "Epoch 10/25, Batch 19488/30044, Loss: 0.007627138867974281\n",
      "Epoch 10/25, Batch 19836/30044, Loss: 0.009064647369086742\n",
      "Epoch 10/25, Batch 20184/30044, Loss: 0.00900981854647398\n",
      "Epoch 10/25, Batch 20532/30044, Loss: 0.010337837040424347\n",
      "Epoch 10/25, Batch 20880/30044, Loss: 0.009119781665503979\n",
      "Epoch 10/25, Batch 21228/30044, Loss: 0.00827827863395214\n",
      "Epoch 10/25, Batch 21576/30044, Loss: 0.010133776813745499\n",
      "Epoch 10/25, Batch 21924/30044, Loss: 0.008147921413183212\n",
      "Epoch 10/25, Batch 22272/30044, Loss: 0.009428047575056553\n",
      "Epoch 10/25, Batch 22620/30044, Loss: 0.010175138711929321\n",
      "Epoch 10/25, Batch 22968/30044, Loss: 0.009099937044084072\n",
      "Epoch 10/25, Batch 23316/30044, Loss: 0.009850190952420235\n",
      "Epoch 10/25, Batch 23664/30044, Loss: 0.010132340714335442\n",
      "Epoch 10/25, Batch 24012/30044, Loss: 0.010936842299997807\n",
      "Epoch 10/25, Batch 24360/30044, Loss: 0.00994203332811594\n",
      "Epoch 10/25, Batch 24708/30044, Loss: 0.009299381636083126\n",
      "Epoch 10/25, Batch 25056/30044, Loss: 0.007773855235427618\n",
      "Epoch 10/25, Batch 25404/30044, Loss: 0.010854228399693966\n",
      "Epoch 10/25, Batch 25752/30044, Loss: 0.00907779112458229\n",
      "Epoch 10/25, Batch 26100/30044, Loss: 0.010586963035166264\n",
      "Epoch 10/25, Batch 26448/30044, Loss: 0.007319189142435789\n",
      "Epoch 10/25, Batch 26796/30044, Loss: 0.012123349122703075\n",
      "Epoch 10/25, Batch 27144/30044, Loss: 0.008945776149630547\n",
      "Epoch 10/25, Batch 27492/30044, Loss: 0.011537938378751278\n",
      "Epoch 10/25, Batch 27840/30044, Loss: 0.006463752128183842\n",
      "Epoch 10/25, Batch 28188/30044, Loss: 0.008545788936316967\n",
      "Epoch 10/25, Batch 28536/30044, Loss: 0.00947833526879549\n",
      "Epoch 10/25, Batch 28884/30044, Loss: 0.009004658088088036\n",
      "Epoch 10/25, Batch 29232/30044, Loss: 0.008865841664373875\n",
      "Epoch 10/25, Batch 29580/30044, Loss: 0.010083334520459175\n",
      "Epoch 10/25, Batch 29928/30044, Loss: 0.00862878654152155\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 35, '_step_count': 36, '_get_lr_called_within_step': False, '_last_lr': [3.454915028125262e-05]}\n",
      "Epoch 11/25, Batch 0/30044, Loss: 0.010112154297530651\n",
      "Epoch 11/25, Batch 348/30044, Loss: 0.00839289277791977\n",
      "Epoch 11/25, Batch 696/30044, Loss: 0.008833959698677063\n",
      "Epoch 11/25, Batch 1044/30044, Loss: 0.008510635234415531\n",
      "Epoch 11/25, Batch 1392/30044, Loss: 0.010197684168815613\n",
      "Epoch 11/25, Batch 1740/30044, Loss: 0.009614340029656887\n",
      "Epoch 11/25, Batch 2088/30044, Loss: 0.00922222901135683\n",
      "Epoch 11/25, Batch 2436/30044, Loss: 0.01095124613493681\n",
      "Epoch 11/25, Batch 2784/30044, Loss: 0.009522398002445698\n",
      "Epoch 11/25, Batch 3132/30044, Loss: 0.007105126976966858\n",
      "Epoch 11/25, Batch 3480/30044, Loss: 0.010258987545967102\n",
      "Epoch 11/25, Batch 3828/30044, Loss: 0.010355028323829174\n",
      "Epoch 11/25, Batch 4176/30044, Loss: 0.007689890451729298\n",
      "Epoch 11/25, Batch 4524/30044, Loss: 0.008217320777475834\n",
      "Epoch 11/25, Batch 4872/30044, Loss: 0.010000773705542088\n",
      "Epoch 11/25, Batch 5220/30044, Loss: 0.00997302494943142\n",
      "Epoch 11/25, Batch 5568/30044, Loss: 0.009044377133250237\n",
      "Epoch 11/25, Batch 5916/30044, Loss: 0.008821647614240646\n",
      "Epoch 11/25, Batch 6264/30044, Loss: 0.010531189851462841\n",
      "Epoch 11/25, Batch 6612/30044, Loss: 0.009683535434305668\n",
      "Epoch 11/25, Batch 6960/30044, Loss: 0.010553358122706413\n",
      "Epoch 11/25, Batch 7308/30044, Loss: 0.009743204340338707\n",
      "Epoch 11/25, Batch 7656/30044, Loss: 0.01060981024056673\n",
      "Epoch 11/25, Batch 8004/30044, Loss: 0.00820204708725214\n",
      "Epoch 11/25, Batch 8352/30044, Loss: 0.009168967604637146\n",
      "Epoch 11/25, Batch 8700/30044, Loss: 0.009253899566829205\n",
      "Epoch 11/25, Batch 9048/30044, Loss: 0.011893530376255512\n",
      "Epoch 11/25, Batch 9396/30044, Loss: 0.00980542041361332\n",
      "Epoch 11/25, Batch 9744/30044, Loss: 0.010777986608445644\n",
      "Epoch 11/25, Batch 10092/30044, Loss: 0.010106322355568409\n",
      "Epoch 11/25, Batch 10440/30044, Loss: 0.010201429948210716\n",
      "Epoch 11/25, Batch 10788/30044, Loss: 0.00843134243041277\n",
      "Epoch 11/25, Batch 11136/30044, Loss: 0.00864339992403984\n",
      "Epoch 11/25, Batch 11484/30044, Loss: 0.010364921763539314\n",
      "Epoch 11/25, Batch 11832/30044, Loss: 0.008867749944329262\n",
      "Epoch 11/25, Batch 12180/30044, Loss: 0.008313075639307499\n",
      "Epoch 11/25, Batch 12528/30044, Loss: 0.0097505459561944\n",
      "Epoch 11/25, Batch 12876/30044, Loss: 0.011582890525460243\n",
      "Epoch 11/25, Batch 13224/30044, Loss: 0.009305507875978947\n",
      "Epoch 11/25, Batch 13572/30044, Loss: 0.010335328057408333\n",
      "Epoch 11/25, Batch 13920/30044, Loss: 0.00840854924172163\n",
      "Epoch 11/25, Batch 14268/30044, Loss: 0.008205220103263855\n",
      "Epoch 11/25, Batch 14616/30044, Loss: 0.00961816031485796\n",
      "Epoch 11/25, Batch 14964/30044, Loss: 0.009425540454685688\n",
      "Epoch 11/25, Batch 15312/30044, Loss: 0.00891801342368126\n",
      "Epoch 11/25, Batch 15660/30044, Loss: 0.011631686240434647\n",
      "Epoch 11/25, Batch 16008/30044, Loss: 0.0077807786874473095\n",
      "Epoch 11/25, Batch 16356/30044, Loss: 0.00891336239874363\n",
      "Epoch 11/25, Batch 16704/30044, Loss: 0.008575479499995708\n",
      "Epoch 11/25, Batch 17052/30044, Loss: 0.00921568926423788\n",
      "Epoch 11/25, Batch 17400/30044, Loss: 0.011034611612558365\n",
      "Epoch 11/25, Batch 17748/30044, Loss: 0.007465209811925888\n",
      "Epoch 11/25, Batch 18096/30044, Loss: 0.009774583391845226\n",
      "Epoch 11/25, Batch 18444/30044, Loss: 0.00807532761245966\n",
      "Epoch 11/25, Batch 18792/30044, Loss: 0.008655400015413761\n",
      "Epoch 11/25, Batch 19140/30044, Loss: 0.008465374819934368\n",
      "Epoch 11/25, Batch 19488/30044, Loss: 0.009193524718284607\n",
      "Epoch 11/25, Batch 19836/30044, Loss: 0.009421457536518574\n",
      "Epoch 11/25, Batch 20184/30044, Loss: 0.009530149400234222\n",
      "Epoch 11/25, Batch 20532/30044, Loss: 0.0114849042147398\n",
      "Epoch 11/25, Batch 20880/30044, Loss: 0.011107821017503738\n",
      "Epoch 11/25, Batch 21228/30044, Loss: 0.009300589561462402\n",
      "Epoch 11/25, Batch 21576/30044, Loss: 0.008501934818923473\n",
      "Epoch 11/25, Batch 21924/30044, Loss: 0.009844070300459862\n",
      "Epoch 11/25, Batch 22272/30044, Loss: 0.007553070317953825\n",
      "Epoch 11/25, Batch 22620/30044, Loss: 0.010838433168828487\n",
      "Epoch 11/25, Batch 22968/30044, Loss: 0.009817833080887794\n",
      "Epoch 11/25, Batch 23316/30044, Loss: 0.00969974510371685\n",
      "Epoch 11/25, Batch 23664/30044, Loss: 0.00841837003827095\n",
      "Epoch 11/25, Batch 24012/30044, Loss: 0.009271315298974514\n",
      "Epoch 11/25, Batch 24360/30044, Loss: 0.009895783849060535\n",
      "Epoch 11/25, Batch 24708/30044, Loss: 0.008767197839915752\n",
      "Epoch 11/25, Batch 25056/30044, Loss: 0.009619391523301601\n",
      "Epoch 11/25, Batch 25404/30044, Loss: 0.008853389881551266\n",
      "Epoch 11/25, Batch 25752/30044, Loss: 0.009534918703138828\n",
      "Epoch 11/25, Batch 26100/30044, Loss: 0.008839810267090797\n",
      "Epoch 11/25, Batch 26448/30044, Loss: 0.007979308255016804\n",
      "Epoch 11/25, Batch 26796/30044, Loss: 0.007485993206501007\n",
      "Epoch 11/25, Batch 27144/30044, Loss: 0.00829792208969593\n",
      "Epoch 11/25, Batch 27492/30044, Loss: 0.008891446515917778\n",
      "Epoch 11/25, Batch 27840/30044, Loss: 0.010016773827373981\n",
      "Epoch 11/25, Batch 28188/30044, Loss: 0.008795212022960186\n",
      "Epoch 11/25, Batch 28536/30044, Loss: 0.00806591659784317\n",
      "Epoch 11/25, Batch 28884/30044, Loss: 0.007503698114305735\n",
      "Epoch 11/25, Batch 29232/30044, Loss: 0.009240733459591866\n",
      "Epoch 11/25, Batch 29580/30044, Loss: 0.008488775230944157\n",
      "Epoch 11/25, Batch 29928/30044, Loss: 0.00934372004121542\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 36, '_step_count': 37, '_get_lr_called_within_step': False, '_last_lr': [4.0630934270713774e-05]}\n",
      "Epoch 12/25, Batch 0/30044, Loss: 0.010198717005550861\n",
      "Epoch 12/25, Batch 348/30044, Loss: 0.010250523686408997\n",
      "Epoch 12/25, Batch 696/30044, Loss: 0.012096134014427662\n",
      "Epoch 12/25, Batch 1044/30044, Loss: 0.008948167786002159\n",
      "Epoch 12/25, Batch 1392/30044, Loss: 0.010762770660221577\n",
      "Epoch 12/25, Batch 1740/30044, Loss: 0.008008677512407303\n",
      "Epoch 12/25, Batch 2088/30044, Loss: 0.007790984585881233\n",
      "Epoch 12/25, Batch 2436/30044, Loss: 0.008468864485621452\n",
      "Epoch 12/25, Batch 2784/30044, Loss: 0.008130056783556938\n",
      "Epoch 12/25, Batch 3132/30044, Loss: 0.00995524413883686\n",
      "Epoch 12/25, Batch 3480/30044, Loss: 0.010605509392917156\n",
      "Epoch 12/25, Batch 3828/30044, Loss: 0.008758578449487686\n",
      "Epoch 12/25, Batch 4176/30044, Loss: 0.007376492023468018\n",
      "Epoch 12/25, Batch 4524/30044, Loss: 0.009232665412127972\n",
      "Epoch 12/25, Batch 4872/30044, Loss: 0.010058094747364521\n",
      "Epoch 12/25, Batch 5220/30044, Loss: 0.009774885140359402\n",
      "Epoch 12/25, Batch 5568/30044, Loss: 0.009417679160833359\n",
      "Epoch 12/25, Batch 5916/30044, Loss: 0.009302209131419659\n",
      "Epoch 12/25, Batch 6264/30044, Loss: 0.00962009746581316\n",
      "Epoch 12/25, Batch 6612/30044, Loss: 0.008855286054313183\n",
      "Epoch 12/25, Batch 6960/30044, Loss: 0.009261230938136578\n",
      "Epoch 12/25, Batch 7308/30044, Loss: 0.007837704382836819\n",
      "Epoch 12/25, Batch 7656/30044, Loss: 0.010138793848454952\n",
      "Epoch 12/25, Batch 8004/30044, Loss: 0.009656407870352268\n",
      "Epoch 12/25, Batch 8352/30044, Loss: 0.007866674102842808\n",
      "Epoch 12/25, Batch 8700/30044, Loss: 0.009297346696257591\n",
      "Epoch 12/25, Batch 9048/30044, Loss: 0.009392036125063896\n",
      "Epoch 12/25, Batch 9396/30044, Loss: 0.009105831384658813\n",
      "Epoch 12/25, Batch 9744/30044, Loss: 0.008937628008425236\n",
      "Epoch 12/25, Batch 10092/30044, Loss: 0.0091340821236372\n",
      "Epoch 12/25, Batch 10440/30044, Loss: 0.008122709579765797\n",
      "Epoch 12/25, Batch 10788/30044, Loss: 0.008894061669707298\n",
      "Epoch 12/25, Batch 11136/30044, Loss: 0.009917575865983963\n",
      "Epoch 12/25, Batch 11484/30044, Loss: 0.007742964196950197\n",
      "Epoch 12/25, Batch 11832/30044, Loss: 0.008860208094120026\n",
      "Epoch 12/25, Batch 12180/30044, Loss: 0.010871593840420246\n",
      "Epoch 12/25, Batch 12528/30044, Loss: 0.009297934360802174\n",
      "Epoch 12/25, Batch 12876/30044, Loss: 0.010445576161146164\n",
      "Epoch 12/25, Batch 13224/30044, Loss: 0.010254899971187115\n",
      "Epoch 12/25, Batch 13572/30044, Loss: 0.010830859653651714\n",
      "Epoch 12/25, Batch 13920/30044, Loss: 0.007751754019409418\n",
      "Epoch 12/25, Batch 14268/30044, Loss: 0.00880451686680317\n",
      "Epoch 12/25, Batch 14616/30044, Loss: 0.009447942487895489\n",
      "Epoch 12/25, Batch 14964/30044, Loss: 0.011886999942362309\n",
      "Epoch 12/25, Batch 15312/30044, Loss: 0.010329099372029305\n",
      "Epoch 12/25, Batch 15660/30044, Loss: 0.010130124166607857\n",
      "Epoch 12/25, Batch 16008/30044, Loss: 0.012324715033173561\n",
      "Epoch 12/25, Batch 16356/30044, Loss: 0.010262816213071346\n",
      "Epoch 12/25, Batch 16704/30044, Loss: 0.008993882685899734\n",
      "Epoch 12/25, Batch 17052/30044, Loss: 0.009863377548754215\n",
      "Epoch 12/25, Batch 17400/30044, Loss: 0.009038884192705154\n",
      "Epoch 12/25, Batch 17748/30044, Loss: 0.010168254375457764\n",
      "Epoch 12/25, Batch 18096/30044, Loss: 0.011174489744007587\n",
      "Epoch 12/25, Batch 18444/30044, Loss: 0.009584594517946243\n",
      "Epoch 12/25, Batch 18792/30044, Loss: 0.00894608348608017\n",
      "Epoch 12/25, Batch 19140/30044, Loss: 0.01001648511737585\n",
      "Epoch 12/25, Batch 19488/30044, Loss: 0.007992186583578587\n",
      "Epoch 12/25, Batch 19836/30044, Loss: 0.010171595960855484\n",
      "Epoch 12/25, Batch 20184/30044, Loss: 0.01111188717186451\n",
      "Epoch 12/25, Batch 20532/30044, Loss: 0.009713776409626007\n",
      "Epoch 12/25, Batch 20880/30044, Loss: 0.010732368566095829\n",
      "Epoch 12/25, Batch 21228/30044, Loss: 0.009100423194468021\n",
      "Epoch 12/25, Batch 21576/30044, Loss: 0.007918749935925007\n",
      "Epoch 12/25, Batch 21924/30044, Loss: 0.008757596835494041\n",
      "Epoch 12/25, Batch 22272/30044, Loss: 0.008852877654135227\n",
      "Epoch 12/25, Batch 22620/30044, Loss: 0.010601930320262909\n",
      "Epoch 12/25, Batch 22968/30044, Loss: 0.009809168986976147\n",
      "Epoch 12/25, Batch 23316/30044, Loss: 0.009714871644973755\n",
      "Epoch 12/25, Batch 23664/30044, Loss: 0.009026579558849335\n",
      "Epoch 12/25, Batch 24012/30044, Loss: 0.011239607818424702\n",
      "Epoch 12/25, Batch 24360/30044, Loss: 0.00923936627805233\n",
      "Epoch 12/25, Batch 24708/30044, Loss: 0.008293132297694683\n",
      "Epoch 12/25, Batch 25056/30044, Loss: 0.008862705901265144\n",
      "Epoch 12/25, Batch 25404/30044, Loss: 0.010290702804923058\n",
      "Epoch 12/25, Batch 25752/30044, Loss: 0.01125239860266447\n",
      "Epoch 12/25, Batch 26100/30044, Loss: 0.009942849166691303\n",
      "Epoch 12/25, Batch 26448/30044, Loss: 0.009911340661346912\n",
      "Epoch 12/25, Batch 26796/30044, Loss: 0.012012087740004063\n",
      "Epoch 12/25, Batch 27144/30044, Loss: 0.010761307552456856\n",
      "Epoch 12/25, Batch 27492/30044, Loss: 0.009322116151452065\n",
      "Epoch 12/25, Batch 27840/30044, Loss: 0.008778618648648262\n",
      "Epoch 12/25, Batch 28188/30044, Loss: 0.00903705507516861\n",
      "Epoch 12/25, Batch 28536/30044, Loss: 0.008279833942651749\n",
      "Epoch 12/25, Batch 28884/30044, Loss: 0.0103659238666296\n",
      "Epoch 12/25, Batch 29232/30044, Loss: 0.008875664323568344\n",
      "Epoch 12/25, Batch 29580/30044, Loss: 0.00890638493001461\n",
      "Epoch 12/25, Batch 29928/30044, Loss: 0.008482810109853745\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 37, '_step_count': 38, '_get_lr_called_within_step': False, '_last_lr': [4.686047402353434e-05]}\n",
      "Epoch 13/25, Batch 0/30044, Loss: 0.008112985640764236\n",
      "Epoch 13/25, Batch 348/30044, Loss: 0.011570736765861511\n",
      "Epoch 13/25, Batch 696/30044, Loss: 0.008773613721132278\n",
      "Epoch 13/25, Batch 1044/30044, Loss: 0.009815402328968048\n",
      "Epoch 13/25, Batch 1392/30044, Loss: 0.00762864388525486\n",
      "Epoch 13/25, Batch 1740/30044, Loss: 0.008352604694664478\n",
      "Epoch 13/25, Batch 2088/30044, Loss: 0.010028739459812641\n",
      "Epoch 13/25, Batch 2436/30044, Loss: 0.010282580740749836\n",
      "Epoch 13/25, Batch 2784/30044, Loss: 0.009883534163236618\n",
      "Epoch 13/25, Batch 3132/30044, Loss: 0.008475873619318008\n",
      "Epoch 13/25, Batch 3480/30044, Loss: 0.008993283845484257\n",
      "Epoch 13/25, Batch 3828/30044, Loss: 0.009256630204617977\n",
      "Epoch 13/25, Batch 4176/30044, Loss: 0.010950484313070774\n",
      "Epoch 13/25, Batch 4524/30044, Loss: 0.008989916183054447\n",
      "Epoch 13/25, Batch 4872/30044, Loss: 0.009372027590870857\n",
      "Epoch 13/25, Batch 5220/30044, Loss: 0.008498694747686386\n",
      "Epoch 13/25, Batch 5568/30044, Loss: 0.008552771992981434\n",
      "Epoch 13/25, Batch 5916/30044, Loss: 0.008157478645443916\n",
      "Epoch 13/25, Batch 6264/30044, Loss: 0.008700147271156311\n",
      "Epoch 13/25, Batch 6612/30044, Loss: 0.010417656041681767\n",
      "Epoch 13/25, Batch 6960/30044, Loss: 0.009252192452549934\n",
      "Epoch 13/25, Batch 7308/30044, Loss: 0.011001459322869778\n",
      "Epoch 13/25, Batch 7656/30044, Loss: 0.007702937815338373\n",
      "Epoch 13/25, Batch 8004/30044, Loss: 0.008487135171890259\n",
      "Epoch 13/25, Batch 8352/30044, Loss: 0.009988508187234402\n",
      "Epoch 13/25, Batch 8700/30044, Loss: 0.010258841328322887\n",
      "Epoch 13/25, Batch 9048/30044, Loss: 0.008415408432483673\n",
      "Epoch 13/25, Batch 9396/30044, Loss: 0.00934632308781147\n",
      "Epoch 13/25, Batch 9744/30044, Loss: 0.009575411677360535\n",
      "Epoch 13/25, Batch 10092/30044, Loss: 0.008824029937386513\n",
      "Epoch 13/25, Batch 10440/30044, Loss: 0.00926165096461773\n",
      "Epoch 13/25, Batch 10788/30044, Loss: 0.010015226900577545\n",
      "Epoch 13/25, Batch 11136/30044, Loss: 0.008457244373857975\n",
      "Epoch 13/25, Batch 11484/30044, Loss: 0.008212354965507984\n",
      "Epoch 13/25, Batch 11832/30044, Loss: 0.00897747091948986\n",
      "Epoch 13/25, Batch 12180/30044, Loss: 0.008770876564085484\n",
      "Epoch 13/25, Batch 12528/30044, Loss: 0.011730881407856941\n",
      "Epoch 13/25, Batch 12876/30044, Loss: 0.009339389391243458\n",
      "Epoch 13/25, Batch 13224/30044, Loss: 0.008985442109405994\n",
      "Epoch 13/25, Batch 13572/30044, Loss: 0.008143247105181217\n",
      "Epoch 13/25, Batch 13920/30044, Loss: 0.008517627604305744\n",
      "Epoch 13/25, Batch 14268/30044, Loss: 0.00974928867071867\n",
      "Epoch 13/25, Batch 14616/30044, Loss: 0.009868430905044079\n",
      "Epoch 13/25, Batch 14964/30044, Loss: 0.008822734467685223\n",
      "Epoch 13/25, Batch 15312/30044, Loss: 0.010088258422911167\n",
      "Epoch 13/25, Batch 15660/30044, Loss: 0.00992755126208067\n",
      "Epoch 13/25, Batch 16008/30044, Loss: 0.00978202372789383\n",
      "Epoch 13/25, Batch 16356/30044, Loss: 0.009507396258413792\n",
      "Epoch 13/25, Batch 16704/30044, Loss: 0.008592919446527958\n",
      "Epoch 13/25, Batch 17052/30044, Loss: 0.010397778823971748\n",
      "Epoch 13/25, Batch 17400/30044, Loss: 0.007235701195895672\n",
      "Epoch 13/25, Batch 17748/30044, Loss: 0.010660617612302303\n",
      "Epoch 13/25, Batch 18096/30044, Loss: 0.009021284058690071\n",
      "Epoch 13/25, Batch 18444/30044, Loss: 0.008898532949388027\n",
      "Epoch 13/25, Batch 18792/30044, Loss: 0.010256988927721977\n",
      "Epoch 13/25, Batch 19140/30044, Loss: 0.008525757119059563\n",
      "Epoch 13/25, Batch 19488/30044, Loss: 0.008633610792458057\n",
      "Epoch 13/25, Batch 19836/30044, Loss: 0.01016616728156805\n",
      "Epoch 13/25, Batch 20184/30044, Loss: 0.009240721352398396\n",
      "Epoch 13/25, Batch 20532/30044, Loss: 0.0087617551907897\n",
      "Epoch 13/25, Batch 20880/30044, Loss: 0.008788727223873138\n",
      "Epoch 13/25, Batch 21228/30044, Loss: 0.008497639559209347\n",
      "Epoch 13/25, Batch 21576/30044, Loss: 0.008913286961615086\n",
      "Epoch 13/25, Batch 21924/30044, Loss: 0.009694745764136314\n",
      "Epoch 13/25, Batch 22272/30044, Loss: 0.008987312205135822\n",
      "Epoch 13/25, Batch 22620/30044, Loss: 0.010010309517383575\n",
      "Epoch 13/25, Batch 22968/30044, Loss: 0.01026039756834507\n",
      "Epoch 13/25, Batch 23316/30044, Loss: 0.007796519435942173\n",
      "Epoch 13/25, Batch 23664/30044, Loss: 0.009541265666484833\n",
      "Epoch 13/25, Batch 24012/30044, Loss: 0.010617513209581375\n",
      "Epoch 13/25, Batch 24360/30044, Loss: 0.008555861189961433\n",
      "Epoch 13/25, Batch 24708/30044, Loss: 0.00893073808401823\n",
      "Epoch 13/25, Batch 25056/30044, Loss: 0.00857516285032034\n",
      "Epoch 13/25, Batch 25404/30044, Loss: 0.008996650576591492\n",
      "Epoch 13/25, Batch 25752/30044, Loss: 0.008164209313690662\n",
      "Epoch 13/25, Batch 26100/30044, Loss: 0.009710553102195263\n",
      "Epoch 13/25, Batch 26448/30044, Loss: 0.010535763576626778\n",
      "Epoch 13/25, Batch 26796/30044, Loss: 0.010659964755177498\n",
      "Epoch 13/25, Batch 27144/30044, Loss: 0.008978869765996933\n",
      "Epoch 13/25, Batch 27492/30044, Loss: 0.008224859833717346\n",
      "Epoch 13/25, Batch 27840/30044, Loss: 0.008794330060482025\n",
      "Epoch 13/25, Batch 28188/30044, Loss: 0.008963847532868385\n",
      "Epoch 13/25, Batch 28536/30044, Loss: 0.009733318351209164\n",
      "Epoch 13/25, Batch 28884/30044, Loss: 0.009677191264927387\n",
      "Epoch 13/25, Batch 29232/30044, Loss: 0.008768073283135891\n",
      "Epoch 13/25, Batch 29580/30044, Loss: 0.01020810753107071\n",
      "Epoch 13/25, Batch 29928/30044, Loss: 0.008930974639952183\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 38, '_step_count': 39, '_get_lr_called_within_step': False, '_last_lr': [5.313952597646565e-05]}\n",
      "Epoch 14/25, Batch 0/30044, Loss: 0.010606255382299423\n",
      "Epoch 14/25, Batch 348/30044, Loss: 0.010538743808865547\n",
      "Epoch 14/25, Batch 696/30044, Loss: 0.00981129426509142\n",
      "Epoch 14/25, Batch 1044/30044, Loss: 0.008408383466303349\n",
      "Epoch 14/25, Batch 1392/30044, Loss: 0.009154445491731167\n",
      "Epoch 14/25, Batch 1740/30044, Loss: 0.009484182111918926\n",
      "Epoch 14/25, Batch 2088/30044, Loss: 0.0094047412276268\n",
      "Epoch 14/25, Batch 2436/30044, Loss: 0.009242200292646885\n",
      "Epoch 14/25, Batch 2784/30044, Loss: 0.00884993001818657\n",
      "Epoch 14/25, Batch 3132/30044, Loss: 0.008850143291056156\n",
      "Epoch 14/25, Batch 3480/30044, Loss: 0.009737545624375343\n",
      "Epoch 14/25, Batch 3828/30044, Loss: 0.011235530488193035\n",
      "Epoch 14/25, Batch 4176/30044, Loss: 0.009828737936913967\n",
      "Epoch 14/25, Batch 4524/30044, Loss: 0.008738008327782154\n",
      "Epoch 14/25, Batch 4872/30044, Loss: 0.008922044187784195\n",
      "Epoch 14/25, Batch 5220/30044, Loss: 0.008532625623047352\n",
      "Epoch 14/25, Batch 5568/30044, Loss: 0.006814699620008469\n",
      "Epoch 14/25, Batch 5916/30044, Loss: 0.00972739327698946\n",
      "Epoch 14/25, Batch 6264/30044, Loss: 0.009361854754388332\n",
      "Epoch 14/25, Batch 6612/30044, Loss: 0.010204899124801159\n",
      "Epoch 14/25, Batch 6960/30044, Loss: 0.009364635683596134\n",
      "Epoch 14/25, Batch 7308/30044, Loss: 0.009313409216701984\n",
      "Epoch 14/25, Batch 7656/30044, Loss: 0.00915842317044735\n",
      "Epoch 14/25, Batch 8004/30044, Loss: 0.009210729971528053\n",
      "Epoch 14/25, Batch 8352/30044, Loss: 0.008384313434362411\n",
      "Epoch 14/25, Batch 8700/30044, Loss: 0.00878104381263256\n",
      "Epoch 14/25, Batch 9048/30044, Loss: 0.00874736625701189\n",
      "Epoch 14/25, Batch 9396/30044, Loss: 0.008794601075351238\n",
      "Epoch 14/25, Batch 9744/30044, Loss: 0.009536362253129482\n",
      "Epoch 14/25, Batch 10092/30044, Loss: 0.00916224718093872\n",
      "Epoch 14/25, Batch 10440/30044, Loss: 0.009271413087844849\n",
      "Epoch 14/25, Batch 10788/30044, Loss: 0.010279075242578983\n",
      "Epoch 14/25, Batch 11136/30044, Loss: 0.008620614185929298\n",
      "Epoch 14/25, Batch 11484/30044, Loss: 0.009151001460850239\n",
      "Epoch 14/25, Batch 11832/30044, Loss: 0.00876358151435852\n",
      "Epoch 14/25, Batch 12180/30044, Loss: 0.009533636271953583\n",
      "Epoch 14/25, Batch 12528/30044, Loss: 0.009723271243274212\n",
      "Epoch 14/25, Batch 12876/30044, Loss: 0.010969382710754871\n",
      "Epoch 14/25, Batch 13224/30044, Loss: 0.008986922912299633\n",
      "Epoch 14/25, Batch 13572/30044, Loss: 0.010848930105566978\n",
      "Epoch 14/25, Batch 13920/30044, Loss: 0.008941574022173882\n",
      "Epoch 14/25, Batch 14268/30044, Loss: 0.009644092060625553\n",
      "Epoch 14/25, Batch 14616/30044, Loss: 0.008225616998970509\n",
      "Epoch 14/25, Batch 14964/30044, Loss: 0.007530834060162306\n",
      "Epoch 14/25, Batch 15312/30044, Loss: 0.008478224277496338\n",
      "Epoch 14/25, Batch 15660/30044, Loss: 0.008383434265851974\n",
      "Epoch 14/25, Batch 16008/30044, Loss: 0.007802484557032585\n",
      "Epoch 14/25, Batch 16356/30044, Loss: 0.008493022061884403\n",
      "Epoch 14/25, Batch 16704/30044, Loss: 0.009328600019216537\n",
      "Epoch 14/25, Batch 17052/30044, Loss: 0.008633099496364594\n",
      "Epoch 14/25, Batch 17400/30044, Loss: 0.009448819793760777\n",
      "Epoch 14/25, Batch 17748/30044, Loss: 0.009443704970180988\n",
      "Epoch 14/25, Batch 18096/30044, Loss: 0.008137769997119904\n",
      "Epoch 14/25, Batch 18444/30044, Loss: 0.009629163891077042\n",
      "Epoch 14/25, Batch 18792/30044, Loss: 0.00985404197126627\n",
      "Epoch 14/25, Batch 19140/30044, Loss: 0.010208518244326115\n",
      "Epoch 14/25, Batch 19488/30044, Loss: 0.009518519975244999\n",
      "Epoch 14/25, Batch 19836/30044, Loss: 0.011177158914506435\n",
      "Epoch 14/25, Batch 20184/30044, Loss: 0.009026054292917252\n",
      "Epoch 14/25, Batch 20532/30044, Loss: 0.008390547707676888\n",
      "Epoch 14/25, Batch 20880/30044, Loss: 0.00891775544732809\n",
      "Epoch 14/25, Batch 21228/30044, Loss: 0.011791190132498741\n",
      "Epoch 14/25, Batch 21576/30044, Loss: 0.009588222950696945\n",
      "Epoch 14/25, Batch 21924/30044, Loss: 0.007816030643880367\n",
      "Epoch 14/25, Batch 22272/30044, Loss: 0.00788991991430521\n",
      "Epoch 14/25, Batch 22620/30044, Loss: 0.00906313955783844\n",
      "Epoch 14/25, Batch 22968/30044, Loss: 0.010148226283490658\n",
      "Epoch 14/25, Batch 23316/30044, Loss: 0.009166749194264412\n",
      "Epoch 14/25, Batch 23664/30044, Loss: 0.008686425164341927\n",
      "Epoch 14/25, Batch 24012/30044, Loss: 0.009589348919689655\n",
      "Epoch 14/25, Batch 24360/30044, Loss: 0.008822173811495304\n",
      "Epoch 14/25, Batch 24708/30044, Loss: 0.009957380592823029\n",
      "Epoch 14/25, Batch 25056/30044, Loss: 0.011664499528706074\n",
      "Epoch 14/25, Batch 25404/30044, Loss: 0.010102114640176296\n",
      "Epoch 14/25, Batch 25752/30044, Loss: 0.00945974513888359\n",
      "Epoch 14/25, Batch 26100/30044, Loss: 0.00879601389169693\n",
      "Epoch 14/25, Batch 26448/30044, Loss: 0.009609906002879143\n",
      "Epoch 14/25, Batch 26796/30044, Loss: 0.008683237247169018\n",
      "Epoch 14/25, Batch 27144/30044, Loss: 0.01118509378284216\n",
      "Epoch 14/25, Batch 27492/30044, Loss: 0.009075830690562725\n",
      "Epoch 14/25, Batch 27840/30044, Loss: 0.00758854765444994\n",
      "Epoch 14/25, Batch 28188/30044, Loss: 0.008515695109963417\n",
      "Epoch 14/25, Batch 28536/30044, Loss: 0.008563338778913021\n",
      "Epoch 14/25, Batch 28884/30044, Loss: 0.011476396583020687\n",
      "Epoch 14/25, Batch 29232/30044, Loss: 0.00963213387876749\n",
      "Epoch 14/25, Batch 29580/30044, Loss: 0.008281122893095016\n",
      "Epoch 14/25, Batch 29928/30044, Loss: 0.00987369753420353\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 39, '_step_count': 40, '_get_lr_called_within_step': False, '_last_lr': [5.936906572928623e-05]}\n",
      "Epoch 15/25, Batch 0/30044, Loss: 0.010248111560940742\n",
      "Epoch 15/25, Batch 348/30044, Loss: 0.008441966027021408\n",
      "Epoch 15/25, Batch 696/30044, Loss: 0.008391541428864002\n",
      "Epoch 15/25, Batch 1044/30044, Loss: 0.00921647995710373\n",
      "Epoch 15/25, Batch 1392/30044, Loss: 0.01083403080701828\n",
      "Epoch 15/25, Batch 1740/30044, Loss: 0.008862607181072235\n",
      "Epoch 15/25, Batch 2088/30044, Loss: 0.008874451741576195\n",
      "Epoch 15/25, Batch 2436/30044, Loss: 0.007990059442818165\n",
      "Epoch 15/25, Batch 2784/30044, Loss: 0.010891901329159737\n",
      "Epoch 15/25, Batch 3132/30044, Loss: 0.008148803375661373\n",
      "Epoch 15/25, Batch 3480/30044, Loss: 0.01053222268819809\n",
      "Epoch 15/25, Batch 3828/30044, Loss: 0.009052117355167866\n",
      "Epoch 15/25, Batch 4176/30044, Loss: 0.0084929708391428\n",
      "Epoch 15/25, Batch 4524/30044, Loss: 0.011663937009871006\n",
      "Epoch 15/25, Batch 4872/30044, Loss: 0.00910178292542696\n",
      "Epoch 15/25, Batch 5220/30044, Loss: 0.009372612461447716\n",
      "Epoch 15/25, Batch 5568/30044, Loss: 0.00965452566742897\n",
      "Epoch 15/25, Batch 5916/30044, Loss: 0.010401129722595215\n",
      "Epoch 15/25, Batch 6264/30044, Loss: 0.010101620107889175\n",
      "Epoch 15/25, Batch 6612/30044, Loss: 0.00797452311962843\n",
      "Epoch 15/25, Batch 6960/30044, Loss: 0.008716115728020668\n",
      "Epoch 15/25, Batch 7308/30044, Loss: 0.009147080592811108\n",
      "Epoch 15/25, Batch 7656/30044, Loss: 0.009939111769199371\n",
      "Epoch 15/25, Batch 8004/30044, Loss: 0.010443191975355148\n",
      "Epoch 15/25, Batch 8352/30044, Loss: 0.010572509840130806\n",
      "Epoch 15/25, Batch 8700/30044, Loss: 0.007610603701323271\n",
      "Epoch 15/25, Batch 9048/30044, Loss: 0.009150366298854351\n",
      "Epoch 15/25, Batch 9396/30044, Loss: 0.009289145469665527\n",
      "Epoch 15/25, Batch 9744/30044, Loss: 0.008619081228971481\n",
      "Epoch 15/25, Batch 10092/30044, Loss: 0.008170145563781261\n",
      "Epoch 15/25, Batch 10440/30044, Loss: 0.009240514598786831\n",
      "Epoch 15/25, Batch 10788/30044, Loss: 0.008877824060618877\n",
      "Epoch 15/25, Batch 11136/30044, Loss: 0.009257151745259762\n",
      "Epoch 15/25, Batch 11484/30044, Loss: 0.01055743359029293\n",
      "Epoch 15/25, Batch 11832/30044, Loss: 0.008231943473219872\n",
      "Epoch 15/25, Batch 12180/30044, Loss: 0.008221820928156376\n",
      "Epoch 15/25, Batch 12528/30044, Loss: 0.011824727058410645\n",
      "Epoch 15/25, Batch 12876/30044, Loss: 0.009353190660476685\n",
      "Epoch 15/25, Batch 13224/30044, Loss: 0.011332091875374317\n",
      "Epoch 15/25, Batch 13572/30044, Loss: 0.009774615056812763\n",
      "Epoch 15/25, Batch 13920/30044, Loss: 0.009420758113265038\n",
      "Epoch 15/25, Batch 14268/30044, Loss: 0.010569853708148003\n",
      "Epoch 15/25, Batch 14616/30044, Loss: 0.010023385286331177\n",
      "Epoch 15/25, Batch 14964/30044, Loss: 0.009974021464586258\n",
      "Epoch 15/25, Batch 15312/30044, Loss: 0.009762294590473175\n",
      "Epoch 15/25, Batch 15660/30044, Loss: 0.007951595820486546\n",
      "Epoch 15/25, Batch 16008/30044, Loss: 0.00787282083183527\n",
      "Epoch 15/25, Batch 16356/30044, Loss: 0.010320089757442474\n",
      "Epoch 15/25, Batch 16704/30044, Loss: 0.008366257883608341\n",
      "Epoch 15/25, Batch 17052/30044, Loss: 0.009976484812796116\n",
      "Epoch 15/25, Batch 17400/30044, Loss: 0.00877191312611103\n",
      "Epoch 15/25, Batch 17748/30044, Loss: 0.0073881689459085464\n",
      "Epoch 15/25, Batch 18096/30044, Loss: 0.009418495930731297\n",
      "Epoch 15/25, Batch 18444/30044, Loss: 0.00924062542617321\n",
      "Epoch 15/25, Batch 18792/30044, Loss: 0.0083584263920784\n",
      "Epoch 15/25, Batch 19140/30044, Loss: 0.009751862846314907\n",
      "Epoch 15/25, Batch 19488/30044, Loss: 0.008458099327981472\n",
      "Epoch 15/25, Batch 19836/30044, Loss: 0.009305831044912338\n",
      "Epoch 15/25, Batch 20184/30044, Loss: 0.008137735538184643\n",
      "Epoch 15/25, Batch 20532/30044, Loss: 0.011240520514547825\n",
      "Epoch 15/25, Batch 20880/30044, Loss: 0.008576393127441406\n",
      "Epoch 15/25, Batch 21228/30044, Loss: 0.010320382192730904\n",
      "Epoch 15/25, Batch 21576/30044, Loss: 0.008273147977888584\n",
      "Epoch 15/25, Batch 21924/30044, Loss: 0.009379188530147076\n",
      "Epoch 15/25, Batch 22272/30044, Loss: 0.009569582529366016\n",
      "Epoch 15/25, Batch 22620/30044, Loss: 0.011481868103146553\n",
      "Epoch 15/25, Batch 22968/30044, Loss: 0.010953252203762531\n",
      "Epoch 15/25, Batch 23316/30044, Loss: 0.011642753146588802\n",
      "Epoch 15/25, Batch 23664/30044, Loss: 0.008351016789674759\n",
      "Epoch 15/25, Batch 24012/30044, Loss: 0.009004986844956875\n",
      "Epoch 15/25, Batch 24360/30044, Loss: 0.008839467540383339\n",
      "Epoch 15/25, Batch 24708/30044, Loss: 0.008322551846504211\n",
      "Epoch 15/25, Batch 25056/30044, Loss: 0.010008667595684528\n",
      "Epoch 15/25, Batch 25404/30044, Loss: 0.008648253977298737\n",
      "Epoch 15/25, Batch 25752/30044, Loss: 0.008706657215952873\n",
      "Epoch 15/25, Batch 26100/30044, Loss: 0.009382696822285652\n",
      "Epoch 15/25, Batch 26448/30044, Loss: 0.00954132154583931\n",
      "Epoch 15/25, Batch 26796/30044, Loss: 0.00739322742447257\n",
      "Epoch 15/25, Batch 27144/30044, Loss: 0.01148285809904337\n",
      "Epoch 15/25, Batch 27492/30044, Loss: 0.009368554688990116\n",
      "Epoch 15/25, Batch 27840/30044, Loss: 0.009255658835172653\n",
      "Epoch 15/25, Batch 28188/30044, Loss: 0.008536283858120441\n",
      "Epoch 15/25, Batch 28536/30044, Loss: 0.009868239052593708\n",
      "Epoch 15/25, Batch 28884/30044, Loss: 0.00881457980722189\n",
      "Epoch 15/25, Batch 29232/30044, Loss: 0.008328338153660297\n",
      "Epoch 15/25, Batch 29580/30044, Loss: 0.010691460222005844\n",
      "Epoch 15/25, Batch 29928/30044, Loss: 0.010737134143710136\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 40, '_step_count': 41, '_get_lr_called_within_step': False, '_last_lr': [6.545084971874737e-05]}\n",
      "Epoch 16/25, Batch 0/30044, Loss: 0.009601830504834652\n",
      "Epoch 16/25, Batch 348/30044, Loss: 0.007973466999828815\n",
      "Epoch 16/25, Batch 696/30044, Loss: 0.008104165084660053\n",
      "Epoch 16/25, Batch 1044/30044, Loss: 0.010736222378909588\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m train_dataset = TrainDataset(train_data_path, train_metadata, transform=transform, grid_length=\u001b[32m0.01\u001b[39m)\n\u001b[32m     35\u001b[39m train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=num_workers)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m           \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m           \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m           \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m           \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Offline/DHBW/semester2/conda/project/src/helpers.py:58\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(model, train_loader, optimizer, device, num_epochs, scheduler, positive_weight_factor)\u001b[39m\n\u001b[32m     56\u001b[39m model.train()\n\u001b[32m     57\u001b[39m criterion = torch.nn.BCEWithLogitsLoss()\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     59\u001b[39m \n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move the data to the device\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Offline/DHBW/semester2/conda/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Offline/DHBW/semester2/conda/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Offline/DHBW/semester2/conda/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1453\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1452\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1453\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1454\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1455\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Offline/DHBW/semester2/conda/project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:111\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[32m    110\u001b[39m     timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:1148\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m     deadline = time.monotonic() + timeout\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[32m   1150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    396\u001b[39m ready = []\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_loop(model=model,\n",
    "           train_loader=train_loader,\n",
    "           optimizer=optimizer,\n",
    "           device=device,\n",
    "           scheduler=scheduler,\n",
    "           num_epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519dacb1",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7a411",
   "metadata": {},
   "source": [
    "### Modell im Evaluierungsmodus speichern und testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c09a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), f\"{model_name}-po-trained\")\n",
    "\n",
    "surveys, top_k_indices = test_loop(model, test_loader, device)\n",
    "\n",
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'surveyId': surveys,\n",
    "        'predictions': data_concatenated,\n",
    "    }\n",
    ").to_csv(f\"csv_submissions/{model_name}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5a2f8",
   "metadata": {},
   "source": [
    "### Laden des PA-Datensatzes und instanziieren des Dataloaders für das Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3046b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training metadata\n",
    "train_data_path = \"data/SatellitePatches/PA-train\"\n",
    "train_metadata_path = \"data/GLC25_PA_metadata_train.csv\"\n",
    "train_metadata = pd.read_csv(train_metadata_path)\n",
    "train_dataset = TrainDataset(train_data_path, train_metadata, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ab20f",
   "metadata": {},
   "source": [
    "### Finetuning mit den PA-Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecc0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model=model,\n",
    "           train_loader=train_loader,\n",
    "           optimizer=optimizer,\n",
    "           device=device,\n",
    "           scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf06b9f",
   "metadata": {},
   "source": [
    "### Speichern des trainierten Modells im Evaluierungmoduss und testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f29a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 88987\n",
      "Training for 25 epochs started.\n",
      "Epoch 1/25, Batch 0/696, Loss: 0.008249456062912941\n",
      "Epoch 1/25, Batch 348/696, Loss: 0.006688857451081276\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 41, '_step_count': 42, '_get_lr_called_within_step': False, '_last_lr': [7.128896457825359e-05]}\n",
      "Epoch 2/25, Batch 0/696, Loss: 0.006024002097547054\n",
      "Epoch 2/25, Batch 348/696, Loss: 0.005029215477406979\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 42, '_step_count': 43, '_get_lr_called_within_step': False, '_last_lr': [7.679133974894983e-05]}\n",
      "Epoch 3/25, Batch 0/696, Loss: 0.005333971697837114\n",
      "Epoch 3/25, Batch 348/696, Loss: 0.005356017965823412\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 43, '_step_count': 44, '_get_lr_called_within_step': False, '_last_lr': [8.187119948743445e-05]}\n",
      "Epoch 4/25, Batch 0/696, Loss: 0.005329010542482138\n",
      "Epoch 4/25, Batch 348/696, Loss: 0.005227837711572647\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 44, '_step_count': 45, '_get_lr_called_within_step': False, '_last_lr': [8.644843137107055e-05]}\n",
      "Epoch 5/25, Batch 0/696, Loss: 0.005219396203756332\n",
      "Epoch 5/25, Batch 348/696, Loss: 0.005641805473715067\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 45, '_step_count': 46, '_get_lr_called_within_step': False, '_last_lr': [9.045084971874735e-05]}\n",
      "Epoch 6/25, Batch 0/696, Loss: 0.004997001029551029\n",
      "Epoch 6/25, Batch 348/696, Loss: 0.004886173643171787\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 46, '_step_count': 47, '_get_lr_called_within_step': False, '_last_lr': [9.381533400219314e-05]}\n",
      "Epoch 7/25, Batch 0/696, Loss: 0.005024912301450968\n",
      "Epoch 7/25, Batch 348/696, Loss: 0.004755698144435883\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 47, '_step_count': 48, '_get_lr_called_within_step': False, '_last_lr': [9.648882429441255e-05]}\n",
      "Epoch 8/25, Batch 0/696, Loss: 0.00465179979801178\n",
      "Epoch 8/25, Batch 348/696, Loss: 0.0052344840951263905\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 48, '_step_count': 49, '_get_lr_called_within_step': False, '_last_lr': [9.842915805643155e-05]}\n",
      "Epoch 9/25, Batch 0/696, Loss: 0.004921968560665846\n",
      "Epoch 9/25, Batch 348/696, Loss: 0.0045210374519228935\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 49, '_step_count': 50, '_get_lr_called_within_step': False, '_last_lr': [9.960573506572388e-05]}\n",
      "Epoch 10/25, Batch 0/696, Loss: 0.004419331904500723\n",
      "Epoch 10/25, Batch 348/696, Loss: 0.004296137019991875\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 50, '_step_count': 51, '_get_lr_called_within_step': False, '_last_lr': [9.999999999999999e-05]}\n",
      "Epoch 11/25, Batch 0/696, Loss: 0.004951534327119589\n",
      "Epoch 11/25, Batch 348/696, Loss: 0.004449010826647282\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 51, '_step_count': 52, '_get_lr_called_within_step': False, '_last_lr': [9.960573506572388e-05]}\n",
      "Epoch 12/25, Batch 0/696, Loss: 0.004748721141368151\n",
      "Epoch 12/25, Batch 348/696, Loss: 0.004555255640298128\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 52, '_step_count': 53, '_get_lr_called_within_step': False, '_last_lr': [9.842915805643154e-05]}\n",
      "Epoch 13/25, Batch 0/696, Loss: 0.004324791021645069\n",
      "Epoch 13/25, Batch 348/696, Loss: 0.00438411720097065\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 53, '_step_count': 54, '_get_lr_called_within_step': False, '_last_lr': [9.648882429441255e-05]}\n",
      "Epoch 14/25, Batch 0/696, Loss: 0.0044286795891821384\n",
      "Epoch 14/25, Batch 348/696, Loss: 0.004362188279628754\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 54, '_step_count': 55, '_get_lr_called_within_step': False, '_last_lr': [9.381533400219318e-05]}\n",
      "Epoch 15/25, Batch 0/696, Loss: 0.004201845731586218\n",
      "Epoch 15/25, Batch 348/696, Loss: 0.004351346287876368\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 55, '_step_count': 56, '_get_lr_called_within_step': False, '_last_lr': [9.045084971874735e-05]}\n",
      "Epoch 16/25, Batch 0/696, Loss: 0.004129156470298767\n",
      "Epoch 16/25, Batch 348/696, Loss: 0.004271159414201975\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 56, '_step_count': 57, '_get_lr_called_within_step': False, '_last_lr': [8.644843137107056e-05]}\n",
      "Epoch 17/25, Batch 0/696, Loss: 0.004181101452559233\n",
      "Epoch 17/25, Batch 348/696, Loss: 0.004203345626592636\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 57, '_step_count': 58, '_get_lr_called_within_step': False, '_last_lr': [8.18711994874345e-05]}\n",
      "Epoch 18/25, Batch 0/696, Loss: 0.004201123490929604\n",
      "Epoch 18/25, Batch 348/696, Loss: 0.004088306333869696\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 58, '_step_count': 59, '_get_lr_called_within_step': False, '_last_lr': [7.679133974894982e-05]}\n",
      "Epoch 19/25, Batch 0/696, Loss: 0.004105859901756048\n",
      "Epoch 19/25, Batch 348/696, Loss: 0.004021502565592527\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 59, '_step_count': 60, '_get_lr_called_within_step': False, '_last_lr': [7.128896457825364e-05]}\n",
      "Epoch 20/25, Batch 0/696, Loss: 0.00402778759598732\n",
      "Epoch 20/25, Batch 348/696, Loss: 0.004141762387007475\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 60, '_step_count': 61, '_get_lr_called_within_step': False, '_last_lr': [6.545084971874742e-05]}\n",
      "Epoch 21/25, Batch 0/696, Loss: 0.004112287424504757\n",
      "Epoch 21/25, Batch 348/696, Loss: 0.004064613953232765\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 61, '_step_count': 62, '_get_lr_called_within_step': False, '_last_lr': [5.936906572928624e-05]}\n",
      "Epoch 22/25, Batch 0/696, Loss: 0.003761456348001957\n",
      "Epoch 22/25, Batch 348/696, Loss: 0.004410492721945047\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 62, '_step_count': 63, '_get_lr_called_within_step': False, '_last_lr': [5.3139525976465704e-05]}\n",
      "Epoch 23/25, Batch 0/696, Loss: 0.003976271487772465\n",
      "Epoch 23/25, Batch 348/696, Loss: 0.004070114344358444\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 63, '_step_count': 64, '_get_lr_called_within_step': False, '_last_lr': [4.686047402353432e-05]}\n",
      "Epoch 24/25, Batch 0/696, Loss: 0.004192561376839876\n",
      "Epoch 24/25, Batch 348/696, Loss: 0.003821159480139613\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 64, '_step_count': 65, '_get_lr_called_within_step': False, '_last_lr': [4.063093427071375e-05]}\n",
      "Epoch 25/25, Batch 0/696, Loss: 0.0038141650147736073\n",
      "Epoch 25/25, Batch 348/696, Loss: 0.004589106887578964\n",
      "Scheduler: {'T_max': 25, 'eta_min': 0.0, 'base_lrs': [0.0001], 'last_epoch': 65, '_step_count': 66, '_get_lr_called_within_step': False, '_last_lr': [3.454915028125265e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [01:04<00:00,  1.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.eval()\n",
    "torch.save(model.state_dict(), f\"{model_name}-po-trained-finetuned\")\n",
    "\n",
    "from src.helpers import test_loop\n",
    "\n",
    "surveys, top_k_indices = test_loop(model, test_loader, device)\n",
    "\n",
    "data_concatenated = [' '.join(map(str, row)) for row in top_k_indices]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'surveyId': surveys,\n",
    "        'predictions': data_concatenated,\n",
    "    }\n",
    ").to_csv(f\"csv_submissions/{model_name}-po-trained-finetuned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19b386",
   "metadata": {},
   "source": [
    "## Evaluierung der in den CSV-Dateien gespeicherten Ergebnisse erfolgt via Kaggle: https://www.kaggle.com/competitions/geolifeclef-2025/submissions"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11565823,
     "sourceId": 91196,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 35265.370426,
   "end_time": "2025-04-12T04:18:24.491981",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-11T18:30:39.121555",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
